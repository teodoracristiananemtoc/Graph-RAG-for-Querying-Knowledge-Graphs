{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FCGhi6KwxsIc"
      },
      "outputs": [],
      "source": [
        "pip install pykeen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PDgxJ_yMx2MR"
      },
      "outputs": [],
      "source": [
        "pip install networkx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wgelhXlkx9lA"
      },
      "outputs": [],
      "source": [
        "pip install llama-index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H8KwSYpgO90y"
      },
      "outputs": [],
      "source": [
        "pip install graphrag==1.0.1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zxzL-n5QyBZf"
      },
      "outputs": [],
      "source": [
        "pip install graspologic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "36cnNZpbyD1f"
      },
      "outputs": [],
      "source": [
        "pip install langchain_openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0DtHSTEgyNj0"
      },
      "outputs": [],
      "source": [
        "pip install nest_asyncio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install lancedb"
      ],
      "metadata": {
        "id": "Ae_AvhmBeVyN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install sparqlwrapper"
      ],
      "metadata": {
        "id": "ND2TIUABeYw2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oKJBxWSCyIdc"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.graph_stores.types import LabelledNode,Relation, EntityNode\n",
        "from graspologic.partition import hierarchical_leiden\n",
        "from llama_index.core import PropertyGraphIndex\n",
        "from IPython.display import Markdown, display\n",
        "from langchain_openai import  OpenAI\n",
        "import os\n",
        "import openai\n",
        "import ast\n",
        "import asyncio\n",
        "import logging\n",
        "import re\n",
        "import time\n",
        "from collections.abc import AsyncGenerator\n",
        "from copy import deepcopy\n",
        "from datetime import datetime, timezone\n",
        "from typing import Any, Optional\n",
        "from uuid import uuid4\n",
        "\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tiktoken\n",
        "from SPARQLWrapper import SPARQLWrapper, JSON\n",
        "\n",
        "from datashaper import AsyncType, NoopVerbCallbacks, VerbCallbacks\n",
        "\n",
        "from graphrag.config.enums import LLMType\n",
        "from graphrag.config.models.summarize_descriptions_config import SummarizeDescriptionsConfig\n",
        "from graphrag.index.graph.extractors.community_reports.schemas import (\n",
        "    CLAIM_DESCRIPTION,\n",
        "    CLAIM_DETAILS,\n",
        "    CLAIM_ID,\n",
        "    CLAIM_STATUS,\n",
        "    CLAIM_SUBJECT,\n",
        "    CLAIM_TYPE,\n",
        "    COMMUNITY_ID,\n",
        "    EDGE_DEGREE,\n",
        "    EDGE_DESCRIPTION,\n",
        "    EDGE_DETAILS,\n",
        "    EDGE_ID,\n",
        "    EDGE_SOURCE,\n",
        "    EDGE_TARGET,\n",
        "    NODE_DEGREE,\n",
        "    NODE_DESCRIPTION,\n",
        "    NODE_DETAILS,\n",
        "    NODE_ID,\n",
        "    NODE_NAME,\n",
        ")\n",
        "from graphrag.index.operations.cluster_graph import cluster_graph\n",
        "from graphrag.index.operations.compute_edge_combined_degree import compute_edge_combined_degree\n",
        "from graphrag.index.operations.create_graph import create_graph\n",
        "from graphrag.index.operations.embed_graph import embed_graph\n",
        "from graphrag.index.operations.layout_graph import layout_graph\n",
        "from graphrag.index.operations.snapshot import snapshot\n",
        "from graphrag.index.operations.summarize_communities import (\n",
        "    prepare_community_reports,\n",
        "    restore_community_hierarchy,\n",
        "    summarize_communities,\n",
        ")\n",
        "from graphrag.model.community_report import CommunityReport\n",
        "from graphrag.model.covariate import Covariate\n",
        "from graphrag.model.entity import Entity\n",
        "from graphrag.model.relationship import Relationship\n",
        "from graphrag.model.text_unit import TextUnit\n",
        "from graphrag.prompts.query.local_search_system_prompt import LOCAL_SEARCH_SYSTEM_PROMPT\n",
        "from graphrag.query.context_builder.builders import ContextBuilderResult, LocalContextBuilder\n",
        "from graphrag.query.context_builder.community_context import build_community_context\n",
        "from graphrag.query.context_builder.conversation_history import ConversationHistory\n",
        "from graphrag.query.context_builder.entity_extraction import (\n",
        "    EntityVectorStoreKey,\n",
        "    map_query_to_entities,\n",
        ")\n",
        "from graphrag.query.context_builder.local_context import (\n",
        "    build_covariates_context,\n",
        "    build_entity_context,\n",
        "    build_relationship_context,\n",
        "    get_candidate_context,\n",
        ")\n",
        "from graphrag.query.context_builder.source_context import (\n",
        "    build_text_unit_context,\n",
        "    count_relationships,\n",
        ")\n",
        "from graphrag.query.input.retrieval.community_reports import get_candidate_communities\n",
        "from graphrag.query.input.retrieval.text_units import get_candidate_text_units\n",
        "from graphrag.query.llm.base import BaseLLM, BaseLLMCallback, BaseTextEmbedding\n",
        "from graphrag.query.llm.oai.chat_openai import ChatOpenAI\n",
        "from graphrag.query.llm.oai.embedding import OpenAIEmbedding\n",
        "from graphrag.query.llm.oai.typing import OpenaiApiType\n",
        "from graphrag.query.llm.text_utils import num_tokens\n",
        "from graphrag.query.structured_search.base import BaseSearch, LocalContextBuilder, SearchResult\n",
        "from graphrag.query.structured_search.local_search.mixed_context import LocalSearchMixedContext\n",
        "from graphrag.query.structured_search.local_search.search import LocalSearch\n",
        "from graphrag.storage.pipeline_storage import PipelineStorage\n",
        "from graphrag.cache.pipeline_cache import PipelineCache\n",
        "from graphrag.cache.noop_pipeline_cache import NoopPipelineCache\n",
        "from graphrag.vector_stores.base import (\n",
        "    BaseVectorStore,\n",
        "    VectorStoreDocument,\n",
        "    VectorStoreSearchResult,\n",
        ")\n",
        "from graphrag.vector_stores.lancedb import LanceDBVectorStore\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "3hUIOnZrrpql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U4Gy9zlWysrG"
      },
      "outputs": [],
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9zrjCKvLy0Ht"
      },
      "outputs": [],
      "source": [
        "from pykeen.datasets import UMLS\n",
        "\n",
        "umls_dataset = UMLS()\n",
        "training_triples = umls_dataset.training.mapped_triples\n",
        "entity_id_to_label = umls_dataset.entity_to_id\n",
        "relation_id_to_label = umls_dataset.relation_to_id\n",
        "id_to_entity = {v: k for k, v in entity_id_to_label.items()}\n",
        "id_to_relation = {v: k for k, v in relation_id_to_label.items()}\n",
        "id_to_entity = {v: k for k, v in umls_dataset.entity_to_id.items()}\n",
        "id_to_relation = {v: k for k, v in umls_dataset.relation_to_id.items()}\n",
        "\n",
        "triples = umls_dataset.training.mapped_triples\n",
        "data = []\n",
        "\n",
        "for triple in triples:\n",
        "    head, relation, tail = triple.tolist()\n",
        "    data.append({\n",
        "        'Subject': id_to_entity[head],\n",
        "        'Predicate': id_to_relation[relation],\n",
        "        'Object': id_to_entity[tail]\n",
        "    })\n",
        "\n",
        "\n",
        "triples = pd.DataFrame(data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VTt645d2y6L3"
      },
      "outputs": [],
      "source": [
        "def prepare_graph_from_triples(triples_df: pd.DataFrame) -> tuple[pd.DataFrame, pd.DataFrame, nx.DiGraph]:\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "\n",
        "    required_columns = {\"Subject\", \"Predicate\", \"Object\"}\n",
        "    if not required_columns.issubset(triples_df.columns):\n",
        "        raise ValueError(f\"Input DataFrame must contain columns: {required_columns}\")\n",
        "\n",
        "\n",
        "    triples_df[\"weight\"] = triples_df.groupby([\"Subject\", \"Object\", \"Predicate\"])[\"Subject\"].transform(\"count\")\n",
        "\n",
        "\n",
        "    unique_nodes = pd.concat([triples_df[\"Subject\"], triples_df[\"Object\"]]).unique()\n",
        "    nodes = pd.DataFrame(unique_nodes, columns=[\"name\"])\n",
        "    nodes[\"id\"] = nodes[\"name\"].apply(lambda _: str(uuid4()))\n",
        "    nodes[\"type\"] = \"entity\"\n",
        "\n",
        "\n",
        "    node_descriptions = {node: [] for node in nodes[\"name\"]}\n",
        "    for _, row in triples_df.iterrows():\n",
        "        subject, predicate, obj = row[\"Subject\"], row[\"Predicate\"], row[\"Object\"]\n",
        "        node_descriptions[subject].append(f\"{predicate} {obj}\")\n",
        "\n",
        "\n",
        "    nodes[\"description\"] = nodes[\"name\"].apply(lambda x: \"; \".join(node_descriptions[x]) if x in node_descriptions else \"\")\n",
        "\n",
        "\n",
        "    edges = triples_df.rename(columns={\n",
        "        \"Subject\": \"source\",\n",
        "        \"Object\": \"target\",\n",
        "        \"Predicate\": \"description\"\n",
        "    }).drop_duplicates(subset=[\"source\", \"target\", \"description\"])\n",
        "    edges[\"id\"] = edges.index.map(lambda _: str(uuid4()))\n",
        "\n",
        "\n",
        "    graph = nx.from_pandas_edgelist(\n",
        "        edges,\n",
        "        source=\"source\",\n",
        "        target=\"target\",\n",
        "        edge_attr=[\"description\"],\n",
        "        create_using=nx.DiGraph()\n",
        "    )\n",
        "\n",
        "    return nodes, edges, graph\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    triples_df = pd.DataFrame(triples)\n",
        "\n",
        "\n",
        "    nodes, edges, graph = prepare_graph_from_triples(triples_df)\n",
        "\n",
        "    print(\"\\nNodes:\")\n",
        "    print(nodes)\n",
        "\n",
        "    print(\"\\nEdges:\")\n",
        "    print(edges)\n",
        "\n",
        "    print(\"\\nDirected Graph Edges:\")\n",
        "    print(list(graph.edges(data=True)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CAGUNGRc0ebk"
      },
      "outputs": [],
      "source": [
        "def prepare_graph_from_triples(triples_df: pd.DataFrame) -> tuple[pd.DataFrame, pd.DataFrame, nx.DiGraph]:\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "\n",
        "    required_columns = {\"Subject\", \"Predicate\", \"Object\"}\n",
        "    if not required_columns.issubset(triples_df.columns):\n",
        "        raise ValueError(f\"Input DataFrame must contain columns: {required_columns}\")\n",
        "\n",
        "\n",
        "    triples_df[\"weight\"] = triples_df.groupby([\"Subject\", \"Object\", \"Predicate\"])[\"Subject\"].transform(\"count\")\n",
        "\n",
        "\n",
        "    unique_nodes = pd.concat([triples_df[\"Subject\"], triples_df[\"Object\"]]).unique()\n",
        "    nodes = pd.DataFrame(unique_nodes, columns=[\"name\"])\n",
        "    nodes[\"id\"] = nodes[\"name\"].apply(lambda _: str(uuid4()))\n",
        "    nodes[\"type\"] = \"entity\"\n",
        "\n",
        "    node_descriptions = {node: [] for node in nodes[\"name\"]}\n",
        "    for _, row in triples_df.iterrows():\n",
        "        subject, predicate, obj = row[\"Subject\"], row[\"Predicate\"], row[\"Object\"]\n",
        "        node_descriptions[subject].append(f\"{predicate} {obj}\")\n",
        "\n",
        "\n",
        "    nodes[\"description\"] = nodes[\"name\"].apply(lambda x: \"; \".join(node_descriptions[x]) if x in node_descriptions else \"\")\n",
        "\n",
        "\n",
        "    edges = triples_df.rename(columns={\n",
        "        \"Subject\": \"source\",\n",
        "        \"Object\": \"target\",\n",
        "        \"Predicate\": \"description\"\n",
        "    }).drop_duplicates(subset=[\"source\", \"target\", \"description\"])\n",
        "    edges[\"id\"] = edges.index.map(lambda _: str(uuid4()))\n",
        "\n",
        "\n",
        "    entity_summaries = summarize_entity_descriptions(nodes)\n",
        "\n",
        "\n",
        "    relationship_summaries = summarize_relationship_descriptions(edges)\n",
        "\n",
        "\n",
        "    base_relationship_edges = _prep_edges(edges, relationship_summaries)\n",
        "\n",
        "\n",
        "    graph = nx.from_pandas_edgelist(\n",
        "        base_relationship_edges,\n",
        "        source=\"source\",\n",
        "        target=\"target\",\n",
        "        edge_attr=[\"description_summary\"],\n",
        "        create_using=nx.DiGraph()\n",
        "    )\n",
        "\n",
        "\n",
        "    base_entity_nodes = _prep_nodes(nodes, entity_summaries, graph)\n",
        "\n",
        "    return base_entity_nodes, base_relationship_edges, graph\n",
        "\n",
        "\n",
        "def summarize_entity_descriptions(nodes: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    summaries = nodes[[\"name\", \"description\"]].copy()\n",
        "    summaries.rename(columns={\"description\": \"description_summary\"}, inplace=True)\n",
        "    return summaries\n",
        "\n",
        "\n",
        "def summarize_relationship_descriptions(edges: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    summaries = edges[[\"source\", \"target\", \"description\"]].copy()\n",
        "    summaries.rename(columns={\"description\": \"description_summary\"}, inplace=True)\n",
        "    return summaries\n",
        "\n",
        "\n",
        "def _prep_nodes(entities: pd.DataFrame, summaries: pd.DataFrame, graph: nx.DiGraph) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    degrees_df = _compute_degree(graph)\n",
        "    entities = entities.drop(columns=[\"description\"], errors=\"ignore\")\n",
        "    nodes = (\n",
        "        entities.merge(summaries, on=\"name\", how=\"left\")\n",
        "        .merge(degrees_df, on=\"name\")\n",
        "        .drop_duplicates(subset=\"name\")\n",
        "        .rename(columns={\"name\": \"title\"})\n",
        "    )\n",
        "    nodes = nodes.loc[nodes[\"title\"].notna()].reset_index(drop=True)\n",
        "    nodes[\"human_readable_id\"] = nodes.index\n",
        "    nodes[\"id\"] = nodes[\"human_readable_id\"].apply(lambda _: str(uuid4()))\n",
        "    return nodes\n",
        "\n",
        "\n",
        "def _prep_edges(relationships: pd.DataFrame, summaries: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Prepare edges by merging summaries and adding unique IDs.\n",
        "\n",
        "    :param relationships: DataFrame containing edge relationships.\n",
        "    :param summaries: DataFrame containing summarized descriptions for edges.\n",
        "    :return: DataFrame with prepared edges.\n",
        "    \"\"\"\n",
        "    edges = (\n",
        "        relationships.drop_duplicates(subset=[\"source\", \"target\", \"description\"])\n",
        "        .merge(summaries, on=[\"source\", \"target\"], how=\"left\")\n",
        "    )\n",
        "    edges[\"human_readable_id\"] = edges.index\n",
        "    edges[\"id\"] = edges[\"human_readable_id\"].apply(lambda _: str(uuid4()))\n",
        "    return edges\n",
        "\n",
        "\n",
        "def _compute_degree(graph: nx.DiGraph) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Compute in-degree and out-degree for nodes in the graph.\n",
        "\n",
        "    :param graph: Directed graph.\n",
        "    :return: DataFrame with in-degree and out-degree for nodes.\n",
        "    \"\"\"\n",
        "    return pd.DataFrame([\n",
        "        {\"name\": node, \"in_degree\": graph.in_degree(node), \"out_degree\": graph.out_degree(node)}\n",
        "        for node in graph.nodes\n",
        "    ])\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    triples_df = pd.DataFrame(triples)\n",
        "\n",
        "    base_entity_nodes, base_relationship_edges, graph = prepare_graph_from_triples(triples_df)\n",
        "\n",
        "    print(\"\\nPrepared Nodes:\")\n",
        "    print(base_entity_nodes)\n",
        "\n",
        "    print(\"\\nPrepared Edges:\")\n",
        "    print(base_relationship_edges)\n",
        "\n",
        "    print(\"\\nGraph Edges:\")\n",
        "    print(list(graph.edges(data=True)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T_VplXvN1ir3"
      },
      "outputs": [],
      "source": [
        "base_entity_nodes[\"degree\"] = base_entity_nodes[\"in_degree\"] + base_entity_nodes[\"out_degree\"]\n",
        "\n",
        "print(base_entity_nodes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WGdM87E07Tia"
      },
      "outputs": [],
      "source": [
        "def create_final_nodes(\n",
        "    base_entity_nodes: pd.DataFrame,\n",
        "    base_relationship_edges: pd.DataFrame,\n",
        "    base_communities: pd.DataFrame,\n",
        "    callbacks: VerbCallbacks,\n",
        "    layout_strategy: dict[str, Any],\n",
        "    embedding_strategy: dict[str, Any] | None = None,\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"All the steps to transform final nodes.\"\"\"\n",
        "    graph = create_graph(base_relationship_edges)\n",
        "    graph_embeddings = None\n",
        "    if embedding_strategy:\n",
        "        graph_embeddings = embed_graph(\n",
        "            graph,\n",
        "            embedding_strategy,\n",
        "        )\n",
        "    layout = layout_graph(\n",
        "        graph,\n",
        "        callbacks,\n",
        "        layout_strategy,\n",
        "        embeddings=graph_embeddings,\n",
        "    )\n",
        "    nodes = base_entity_nodes.merge(\n",
        "        layout, left_on=\"title\", right_on=\"label\", how=\"left\"\n",
        "    )\n",
        "\n",
        "    joined = nodes.merge(base_communities, on=\"title\", how=\"left\")\n",
        "    joined[\"level\"] = joined[\"level\"].fillna(0).astype(int)\n",
        "    joined[\"community\"] = joined[\"community\"].fillna(-1).astype(int)\n",
        "\n",
        "    return joined.loc[\n",
        "        :,\n",
        "        [\n",
        "            \"id\",\n",
        "            \"human_readable_id\",\n",
        "            \"title\",\n",
        "            \"community\",\n",
        "            \"level\",\n",
        "            \"degree\",\n",
        "            \"x\",\n",
        "            \"y\",\n",
        "        ],\n",
        "    ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xxarnt4PnpQN"
      },
      "outputs": [],
      "source": [
        "async def compute_communities(\n",
        "    base_relationship_edges: pd.DataFrame,\n",
        "    storage: PipelineStorage,\n",
        "    clustering_strategy: dict[str, Any],\n",
        "    snapshot_transient_enabled: bool = False,\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Compute communities based on the graph and clustering strategy.\n",
        "    \"\"\"\n",
        "    graph = create_graph(base_relationship_edges)\n",
        "\n",
        "    communities = cluster_graph(\n",
        "        graph,\n",
        "        strategy=clustering_strategy,\n",
        "    )\n",
        "\n",
        "    base_communities = pd.DataFrame(\n",
        "        communities, columns=pd.Index([\"level\", \"community\", \"parent\", \"title\"])\n",
        "    ).explode(\"title\")\n",
        "    base_communities[\"community\"] = base_communities[\"community\"].astype(int)\n",
        "\n",
        "    if snapshot_transient_enabled:\n",
        "        await snapshot(\n",
        "            base_communities,\n",
        "            name=\"base_communities\",\n",
        "            storage=storage,\n",
        "            formats=[\"parquet\"],\n",
        "        )\n",
        "\n",
        "    return base_communities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mZFtPkxWnkIS"
      },
      "outputs": [],
      "source": [
        "clustering_strategy = {\n",
        "        \"algorithm\": \"leiden\",\n",
        "        \"params\": {\n",
        "            \"resolution\": 1.0,\n",
        "        },\n",
        "}\n",
        "\n",
        "storage = None\n",
        "\n",
        "base_communities = asyncio.run(\n",
        "        compute_communities(\n",
        "            base_relationship_edges=edges,\n",
        "            storage=storage,\n",
        "            clustering_strategy=clustering_strategy,\n",
        "            snapshot_transient_enabled=False,\n",
        "        )\n",
        ")\n",
        "\n",
        "print(\"\\nComputed Communities:\")\n",
        "print(base_communities)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Otq-td14Mri4"
      },
      "outputs": [],
      "source": [
        "clustering_strategy = {\n",
        "        \"algorithm\": \"leiden\",\n",
        "        \"params\": {\n",
        "            \"resolution\": 1.0,\n",
        "        },\n",
        "}\n",
        "strategy= {\"type\": \"leiden\"}\n",
        "storage = None\n",
        "\n",
        "base_communities = asyncio.run(\n",
        "        compute_communities(\n",
        "            base_relationship_edges=edges,\n",
        "            storage=storage,\n",
        "            clustering_strategy=strategy,\n",
        "            snapshot_transient_enabled=False,\n",
        "        )\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gD4XNLOBn1Ca"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tuDHJMeb7yV4"
      },
      "outputs": [],
      "source": [
        "final_nodes = create_final_nodes(\n",
        "        base_entity_nodes=base_entity_nodes,\n",
        "        base_relationship_edges=base_relationship_edges,\n",
        "        base_communities=base_communities,\n",
        "        callbacks=NoopVerbCallbacks(),\n",
        "        layout_strategy={\"type\": \"zero\"},\n",
        "        embedding_strategy=None,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "35QOp6IGobVE"
      },
      "outputs": [],
      "source": [
        "def create_final_entities(base_entity_nodes: pd.DataFrame):\n",
        "    \"\"\"\"\"\"\n",
        "    return base_entity_nodes.loc[\n",
        "        :,\n",
        "        [\n",
        "            \"id\",\n",
        "            \"human_readable_id\",\n",
        "            \"title\",\n",
        "            \"type\",\n",
        "            \"description\",\n",
        "        ],\n",
        "    ]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SrdAZteqoldT"
      },
      "outputs": [],
      "source": [
        "base_entity_nodes[\"description\"]=base_entity_nodes[\"description_summary\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jwxKVpaJ1Sfu"
      },
      "outputs": [],
      "source": [
        "base_relationship_edges[\"description\"]=base_relationship_edges[\"description_summary\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SKJrrGkhoeP1"
      },
      "outputs": [],
      "source": [
        "final_entities= create_final_entities(base_entity_nodes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O0whnlmSokoq"
      },
      "outputs": [],
      "source": [
        "final_entities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lEHyx8Bio7Aa"
      },
      "outputs": [],
      "source": [
        "final_nodes = final_nodes.merge(\n",
        "    final_entities[['id', 'description']],\n",
        "    on='id',\n",
        "    how='left'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z5TH0rpkpwNC"
      },
      "outputs": [],
      "source": [
        "def create_final_relationships(\n",
        "    base_relationship_edges: pd.DataFrame,\n",
        "    base_entity_nodes: pd.DataFrame,\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"All the steps to transform final relationships.\"\"\"\n",
        "    relationships = base_relationship_edges\n",
        "    relationships[\"combined_degree\"] = compute_edge_combined_degree(\n",
        "        relationships,\n",
        "        base_entity_nodes,\n",
        "        node_name_column=\"title\",\n",
        "        node_degree_column=\"degree\",\n",
        "        edge_source_column=\"source\",\n",
        "        edge_target_column=\"target\",\n",
        "    )\n",
        "\n",
        "    return relationships.loc[\n",
        "        :,\n",
        "        [\n",
        "            \"id\",\n",
        "            \"human_readable_id\",\n",
        "            \"source\",\n",
        "            \"target\",\n",
        "            \"description\",\n",
        "            \"weight\",\n",
        "            \"combined_degree\",\n",
        "        ],\n",
        "    ]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9dSl9SP3pxGj"
      },
      "outputs": [],
      "source": [
        "final_relationships = create_final_relationships(base_relationship_edges, base_entity_nodes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sM_Bh-oXp4lx"
      },
      "outputs": [],
      "source": [
        "final_relationships"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KZSaLL-iqIAj"
      },
      "outputs": [],
      "source": [
        "def create_final_nodes(\n",
        "    base_entity_nodes: pd.DataFrame,\n",
        "    base_relationship_edges: pd.DataFrame,\n",
        "    base_communities: pd.DataFrame,\n",
        "    callbacks: VerbCallbacks,\n",
        "    layout_strategy: dict[str, Any],\n",
        "    embedding_strategy: dict[str, Any] | None = None,\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"All the steps to transform final nodes.\"\"\"\n",
        "    graph = create_graph(base_relationship_edges)\n",
        "    graph_embeddings = None\n",
        "    if embedding_strategy:\n",
        "        graph_embeddings = embed_graph(\n",
        "            graph,\n",
        "            embedding_strategy,\n",
        "        )\n",
        "    layout = layout_graph(\n",
        "        graph,\n",
        "        callbacks,\n",
        "        layout_strategy,\n",
        "        embeddings=graph_embeddings,\n",
        "    )\n",
        "    nodes = base_entity_nodes.merge(\n",
        "        layout, left_on=\"title\", right_on=\"label\", how=\"left\"\n",
        "    )\n",
        "\n",
        "    joined = nodes.merge(base_communities, on=\"title\", how=\"left\")\n",
        "    joined[\"level\"] = joined[\"level\"].fillna(0).astype(int)\n",
        "    joined[\"community\"] = joined[\"community\"].fillna(-1).astype(int)\n",
        "\n",
        "    return joined.loc[\n",
        "        :,\n",
        "        [\n",
        "            \"id\",\n",
        "            \"human_readable_id\",\n",
        "            \"title\",\n",
        "            \"community\",\n",
        "            \"level\",\n",
        "            \"degree\",\n",
        "            \"x\",\n",
        "            \"y\",\n",
        "        ],\n",
        "    ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W_wnUWi3qsfn"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zwSeYzZbqbXv"
      },
      "outputs": [],
      "source": [
        "base_communities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8MODGkKcr4M8"
      },
      "outputs": [],
      "source": [
        "base_entity_nodes[\"title\"] = base_entity_nodes[\"title\"].str.strip().str.lower()\n",
        "base_communities[\"title\"] = base_communities[\"title\"].str.strip().str.lower()\n",
        "\n",
        "entity_ids = base_communities.merge(base_entity_nodes, on=\"title\", how=\"inner\")\n",
        "\n",
        "print(\"\\nMerged Entity IDs (after normalization):\")\n",
        "print(entity_ids)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-YH8KSPsr7xa"
      },
      "outputs": [],
      "source": [
        "final_nodes = create_final_nodes(\n",
        "        base_entity_nodes=base_entity_nodes,\n",
        "        base_relationship_edges=base_relationship_edges,\n",
        "        base_communities=base_communities,\n",
        "        callbacks=NoopVerbCallbacks(),\n",
        "        layout_strategy={\"type\": \"zero\"},\n",
        "        embedding_strategy=None,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AK9JMjCzsHqZ"
      },
      "outputs": [],
      "source": [
        "final_nodes = final_nodes.merge(\n",
        "    final_entities[['id', 'description']],\n",
        "    on='id',\n",
        "    how='left'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MfHsb2t8OwM1"
      },
      "outputs": [],
      "source": [
        "def _prep_nodes(input: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\"\"\"\n",
        "\n",
        "    input = input.loc[input.loc[:, COMMUNITY_ID] != -1]\n",
        "\n",
        "\n",
        "    input.loc[:, NODE_DESCRIPTION] = input.loc[:, NODE_DESCRIPTION].fillna(\n",
        "        \"No Description\"\n",
        "    )\n",
        "\n",
        "\n",
        "    input.loc[:, NODE_DETAILS] = input.loc[\n",
        "        :, [NODE_ID, NODE_NAME, NODE_DESCRIPTION, NODE_DEGREE]\n",
        "    ].to_dict(orient=\"records\")\n",
        "\n",
        "    return input\n",
        "\n",
        "\n",
        "def _prep_edges(input: pd.DataFrame) -> pd.DataFrame:\n",
        "    input.fillna(value={NODE_DESCRIPTION: \"No Description\"}, inplace=True)\n",
        "    input.loc[:, EDGE_DETAILS] = input.loc[\n",
        "        :, [EDGE_ID, EDGE_SOURCE, EDGE_TARGET, EDGE_DESCRIPTION, EDGE_DEGREE]\n",
        "    ].to_dict(orient=\"records\")\n",
        "\n",
        "    return input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WHf8H9cbp6oI"
      },
      "outputs": [],
      "source": [
        "com_nodes = _prep_nodes(final_nodes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rNw-org6sOFy"
      },
      "outputs": [],
      "source": [
        "rel2 = _prep_edges(final_relationships)\n",
        "print(rel2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ObiLhpl_sSi3"
      },
      "outputs": [],
      "source": [
        "local_contexts = prepare_community_reports(\n",
        "        com_nodes,\n",
        "        rel2,\n",
        "        None,\n",
        "        NoopVerbCallbacks(),\n",
        "        1000\n",
        ")\n",
        "print(local_contexts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sceKHcE4sYe9"
      },
      "outputs": [],
      "source": [
        "community_hierarchy = restore_community_hierarchy(com_nodes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "96TcBnehtB7x"
      },
      "outputs": [],
      "source": [
        "MOCK_LLM_CONFIG = {\n",
        "    \"type\":  LLMType.OpenAIChat,\n",
        "    \"parse_json\": True,\n",
        "    \"model\": \"gpt-3.5-turbo\",\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "summarize_config = SummarizeDescriptionsConfig(\n",
        "    strategy={\n",
        "        \"type\": \"graph_intelligence\",\n",
        "        \"llm\": MOCK_LLM_CONFIG,\n",
        "        \"max_summary_length\": 2000,\n",
        "    }\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "llt5i6frtNPa"
      },
      "outputs": [],
      "source": [
        "resolved = summarize_config.resolved_strategy(root_dir=\"path/to/root\")\n",
        "print(resolved)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RK_wh8LKs3XR"
      },
      "outputs": [],
      "source": [
        "\n",
        "community_reports = await summarize_communities(\n",
        "        local_contexts,\n",
        "        com_nodes,\n",
        "        community_hierarchy,\n",
        "        NoopVerbCallbacks(),\n",
        "        NoopPipelineCache(),\n",
        "        resolved,\n",
        "        async_mode=\"asyncio\",\n",
        "        num_threads=4,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6lL3RsTEtj-5"
      },
      "outputs": [],
      "source": [
        "def create_final_communities(\n",
        "    base_entity_nodes: pd.DataFrame,\n",
        "    base_relationship_edges: pd.DataFrame,\n",
        "    base_communities: pd.DataFrame,\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Transform final communities by aggregating entity and relationship IDs.\n",
        "    \"\"\"\n",
        "\n",
        "    entity_ids = base_communities.merge(\n",
        "        base_entity_nodes, on=\"title\", how=\"inner\"\n",
        "    )\n",
        "    entity_ids = (\n",
        "        entity_ids.groupby(\"community\")\n",
        "        .agg(entity_ids=(\"id\", list))\n",
        "        .reset_index()\n",
        "    )\n",
        "    max_level = base_communities[\"level\"].max()\n",
        "    all_grouped = pd.DataFrame(\n",
        "        columns=[\"community\", \"level\", \"relationship_ids\"]\n",
        "    )\n",
        "\n",
        "    for level in range(max_level + 1):\n",
        "        communities_at_level = base_communities.loc[base_communities[\"level\"] == level]\n",
        "\n",
        "\n",
        "        sources = base_relationship_edges.merge(\n",
        "            communities_at_level, left_on=\"source\", right_on=\"title\", how=\"inner\"\n",
        "        )\n",
        "        targets = sources.merge(\n",
        "            communities_at_level, left_on=\"target\", right_on=\"title\", how=\"inner\", suffixes=(\"_source\", \"_target\")\n",
        "        )\n",
        "        matched = targets.loc[targets[\"community_source\"] == targets[\"community_target\"]]\n",
        "\n",
        "\n",
        "        grouped = (\n",
        "            matched.groupby(\n",
        "                [\"community_source\", \"level_source\", \"parent_source\"], as_index=False\n",
        "            )\n",
        "            .agg(relationship_ids=(\"id\", list))\n",
        "        )\n",
        "        grouped.rename(\n",
        "            columns={\n",
        "                \"community_source\": \"community\",\n",
        "                \"level_source\": \"level\",\n",
        "                \"parent_source\": \"parent\",\n",
        "            },\n",
        "            inplace=True,\n",
        "        )\n",
        "\n",
        "        all_grouped = pd.concat(\n",
        "            [all_grouped, grouped], ignore_index=True\n",
        "        )\n",
        "\n",
        "    all_grouped[\"relationship_ids\"] = all_grouped[\"relationship_ids\"].apply(\n",
        "        lambda x: sorted(set(x)) if isinstance(x, list) else []\n",
        "    )\n",
        "\n",
        "\n",
        "    communities = all_grouped.merge(entity_ids, on=\"community\", how=\"inner\")\n",
        "\n",
        "\n",
        "    communities[\"id\"] = [str(uuid4()) for _ in range(len(communities))]\n",
        "    communities[\"human_readable_id\"] = communities[\"community\"]\n",
        "    communities[\"title\"] = \"Community \" + communities[\"community\"].astype(str)\n",
        "    communities[\"parent\"] = communities[\"parent\"].astype(int)\n",
        "\n",
        "\n",
        "    communities[\"period\"] = datetime.now(timezone.utc).date().isoformat()\n",
        "    communities[\"size\"] = communities[\"entity_ids\"].apply(len)\n",
        "\n",
        "    return communities[\n",
        "        [\n",
        "            \"id\",\n",
        "            \"human_readable_id\",\n",
        "            \"community\",\n",
        "            \"parent\",\n",
        "            \"level\",\n",
        "            \"title\",\n",
        "            \"entity_ids\",\n",
        "            \"relationship_ids\",\n",
        "            \"period\",\n",
        "            \"size\",\n",
        "        ]\n",
        "    ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8vt3U1atyrf7"
      },
      "outputs": [],
      "source": [
        "final_communities = create_final_communities(\n",
        "        base_entity_nodes,\n",
        "        base_relationship_edges,\n",
        "        base_communities,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wtWHuikO5V06"
      },
      "outputs": [],
      "source": [
        "async def create_final_community_reports(\n",
        "    nodes_input: pd.DataFrame,\n",
        "    edges_input: pd.DataFrame,\n",
        "    entities: pd.DataFrame,\n",
        "    communities: pd.DataFrame,\n",
        "    claims_input: pd.DataFrame | None,\n",
        "    callbacks: VerbCallbacks,\n",
        "    cache: None,\n",
        "    summarization_strategy: dict,\n",
        "    async_mode: AsyncType = AsyncType.AsyncIO,\n",
        "    num_threads: int = 4,\n",
        ") :\n",
        "    \"\"\"All the steps to transform community reports.\"\"\"\n",
        "\n",
        "\n",
        "    community_reports[\"human_readable_id\"] = community_reports[\"community\"]\n",
        "    community_reports[\"id\"] = [uuid4().hex for _ in range(len(community_reports))]\n",
        "    print(community_reports)\n",
        "\n",
        "    merged = community_reports.merge(\n",
        "        communities.loc[:, [\"community\", \"parent\", \"size\", \"period\"]],\n",
        "        on=\"community\",\n",
        "        how=\"left\",\n",
        "        copy=False,\n",
        "    )\n",
        "    return merged.loc[\n",
        "        :,\n",
        "        [\n",
        "            \"id\",\n",
        "            \"human_readable_id\",\n",
        "            \"community\",\n",
        "            \"parent\",\n",
        "            \"level\",\n",
        "            \"title\",\n",
        "            \"summary\",\n",
        "            \"full_content\",\n",
        "            \"rank\",\n",
        "            \"rank_explanation\",\n",
        "            \"findings\",\n",
        "            \"full_content_json\",\n",
        "            \"period\",\n",
        "            \"size\",\n",
        "        ],\n",
        "    ]\n",
        "\n",
        "\n",
        "final_report = asyncio.run(\n",
        "    create_final_community_reports(\n",
        "        final_nodes,\n",
        "        final_relationships,\n",
        "        final_entities,\n",
        "        final_communities,\n",
        "        None,\n",
        "        NoopVerbCallbacks(),\n",
        "        None,\n",
        "        resolved,\n",
        "        async_mode=\"asyncio\",\n",
        "        num_threads=4,\n",
        "    )\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KgmDVo5b0j6p"
      },
      "outputs": [],
      "source": [
        "final_report.rename(columns={'community': 'community_id'}, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s8gi06u30ZYE"
      },
      "outputs": [],
      "source": [
        "def create_community_reports(df: pd.DataFrame) -> list:\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    community_reports = []\n",
        "    for _, d in df.iterrows():\n",
        "\n",
        "        community_report = CommunityReport(\n",
        "            id=d['id'],\n",
        "               short_id=d['human_readable_id'],\n",
        "            title=d['title'],\n",
        "            community_id=d['community_id'],\n",
        "            summary=d['summary'],\n",
        "            full_content=d['full_content'],\n",
        "            rank=d['rank'],\n",
        "            size=d.get('size', None),\n",
        "            period=d.get('period', None)\n",
        "        )\n",
        "        community_reports.append(community_report)\n",
        "    return community_reports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qx4FVNT50btB"
      },
      "outputs": [],
      "source": [
        "com_report = create_community_reports(final_report)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jopRGBqQyvUt"
      },
      "outputs": [],
      "source": [
        "\n",
        "def create_rel_reports(df: pd.DataFrame) -> list:\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    rel_reports = []\n",
        "    for _, d in df.iterrows():\n",
        "\n",
        "        rel_report = Relationship(\n",
        "             id=d['id'],\n",
        "               short_id=d['human_readable_id'],\n",
        "            source=d['source'],\n",
        "            target=d['target'],\n",
        "            description=d.get('description'),\n",
        "            weight=d.get('weight'),\n",
        "        )\n",
        "        rel_reports.append(rel_report)\n",
        "    return rel_reports\n",
        "\n",
        "com_relations = create_rel_reports(final_relationships)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N8T-VRX59QkG"
      },
      "outputs": [],
      "source": [
        "\n",
        "def create_community_entities(df: pd.DataFrame) -> list:\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    community_en = []\n",
        "    for _, d in df.iterrows():\n",
        "        en = Entity(\n",
        "            id=d['id'],\n",
        "               short_id=d['human_readable_id'],\n",
        "            title=d['title'],\n",
        "            type=d['type'],\n",
        "            description = d['description']\n",
        "        )\n",
        "        community_en.append(en)\n",
        "    return community_en\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JQ4eYpFs9Prz"
      },
      "outputs": [],
      "source": [
        "com_entities = create_community_entities(final_entities)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "953YRSuVbc2_"
      },
      "outputs": [],
      "source": [
        "communities_df=final_communities\n",
        "entity_to_community = {}\n",
        "relationship_to_community = {}\n",
        "\n",
        "for _, row in communities_df.iterrows():\n",
        "    community_id = row[\"community\"]\n",
        "    for entity_id in row[\"entity_ids\"]:\n",
        "        if entity_id not in entity_to_community:\n",
        "            entity_to_community[entity_id] = []\n",
        "        entity_to_community[entity_id].append(community_id)\n",
        "    for relationship_id in row[\"relationship_ids\"]:\n",
        "        if relationship_id not in relationship_to_community:\n",
        "            relationship_to_community[relationship_id] = []\n",
        "        relationship_to_community[relationship_id].append(community_id)\n",
        "\n",
        "for entity in com_entities:\n",
        "    if entity.id in entity_to_community:\n",
        "        entity.community_ids = entity_to_community[entity.id]\n",
        "\n",
        "\n",
        "for relationship in com_relations:\n",
        "    if relationship.id in relationship_to_community:\n",
        "        relationship.community_ids = relationship_to_community[relationship.id]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mp_WTINP0B1X"
      },
      "outputs": [],
      "source": [
        "token_encoder = tiktoken.get_encoding(\"cl100k_base\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ybnOeJLdsLjU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lz6VKNTV-Kvn"
      },
      "outputs": [],
      "source": [
        "api_key = os.environ[\"OPENAI_API_KEY\"]\n",
        "llm_model = \"gpt-3.5-turbo\"\n",
        "embedding_model = \"text-embedding-ada-002\"\n",
        "\n",
        "llm2 = ChatOpenAI(\n",
        "    api_key=api_key,\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    api_type=OpenaiApiType.OpenAI,\n",
        "    max_retries=20,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WzdaOfSOISWc"
      },
      "outputs": [],
      "source": [
        "api_key = os.environ[\"OPENAI_API_KEY\"]\n",
        "embedding_model = \"text-embedding-ada-002\"\n",
        "\n",
        "\n",
        "text_embedder = OpenAIEmbedding(\n",
        "    api_key=api_key,\n",
        "    api_base=None,\n",
        "    api_type=OpenaiApiType.OpenAI,\n",
        "    model=embedding_model,\n",
        "    deployment_name=embedding_model,\n",
        "    max_retries=20,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eu3s6AgXNqls"
      },
      "outputs": [],
      "source": [
        "final_relationships = com_relations\n",
        "final_entities = com_entities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQGOSx0KK7E4"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6-TbtRfcemQb"
      },
      "outputs": [],
      "source": [
        "LANCEDB_URI = \"./lancedb_store\"\n",
        "description_embedding_store = LanceDBVectorStore(collection_name=\"entity_descriptions\")\n",
        "description_embedding_store.connect(db_uri=LANCEDB_URI)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1taLYvu4eoo0"
      },
      "outputs": [],
      "source": [
        "\n",
        "entity_text_embeddings = {\n",
        "    entity.id: text_embedder.embed(entity.title) for entity in com_entities\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ePSUxivEJ41a"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "documents = [\n",
        "    VectorStoreDocument(\n",
        "        id=entity.title,\n",
        "        text=entity.title,\n",
        "        vector=text_embedder.embed(entity.title),\n",
        "    )\n",
        "    for entity in com_entities\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LL0MftphexRY"
      },
      "outputs": [],
      "source": [
        "description_embedding_store.load_documents(documents, overwrite=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QSlXzT-kz_Wm"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "local_context_params = {\n",
        "    \"text_unit_prop\": 0.2,\n",
        "    \"community_prop\": 0.8,\n",
        "    \"conversation_history_max_turns\": 5,\n",
        "    \"conversation_history_user_turns_only\": True,\n",
        "    \"top_k_mapped_entities\": 20,\n",
        "    \"top_k_relationships\": 20,\n",
        "    \"include_entity_rank\": True,\n",
        "    \"include_relationship_weight\": True,\n",
        "    \"include_community_rank\": False,\n",
        "    \"return_candidate_context\": False,\n",
        "    \"embedding_vectorstore_key\": EntityVectorStoreKey.ID,\n",
        "    \"max_tokens\": 1000,\n",
        "}\n",
        "\n",
        "llm_params = {\n",
        "    \"max_tokens\": 1000,\n",
        "    \"temperature\": 0.0,\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "la8EX4wfeDVD"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sVs6VCowyAir"
      },
      "outputs": [],
      "source": [
        "\n",
        "def stringify_community_report_fields(reports):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    stringified_reports = []\n",
        "    for report in reports:\n",
        "\n",
        "        stringified_report = CommunityReport(\n",
        "            id=str(report.id),\n",
        "            short_id=str(report.short_id),\n",
        "            title=str(report.title),\n",
        "            community_id=str(report.community_id),\n",
        "            summary=str(report.summary),\n",
        "            full_content=str(report.full_content),\n",
        "            rank=report.rank,\n",
        "            full_content_embedding=str(report.full_content_embedding)\n",
        "            if report.full_content_embedding is not None\n",
        "            else \"\",\n",
        "            attributes={str(k): str(v) for k, v in report.attributes.items()}\n",
        "            if report.attributes\n",
        "            else {},\n",
        "            size=str(report.size),\n",
        "            period=str(report.period),\n",
        "        )\n",
        "        stringified_reports.append(stringified_report)\n",
        "    return stringified_reports\n",
        "\n",
        "\n",
        "\n",
        "com_report_stringified = stringify_community_report_fields(com_report)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WoJcnwK3y7Nv"
      },
      "outputs": [],
      "source": [
        "def stringify_entity_fields(entities):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    stringified_entities = []\n",
        "    for entity in entities:\n",
        "        stringified_entity = Entity(\n",
        "            id=str(entity.id),\n",
        "            short_id=str(entity.short_id),\n",
        "            title=str(entity.title),\n",
        "            type=str(entity.type),\n",
        "            description=str(entity.description),\n",
        "            description_embedding=str(entity.description_embedding)\n",
        "            if entity.description_embedding is not None\n",
        "            else \"\",\n",
        "            name_embedding=str(entity.name_embedding)\n",
        "            if entity.name_embedding is not None\n",
        "            else \"\",\n",
        "            community_ids=[str(community_id) for community_id in entity.community_ids]\n",
        "            if entity.community_ids\n",
        "            else [],\n",
        "            text_unit_ids=[str(text_unit_id) for text_unit_id in entity.text_unit_ids]\n",
        "            if entity.text_unit_ids\n",
        "            else [],\n",
        "            rank=str(entity.rank),\n",
        "            attributes={str(k): str(v) for k, v in entity.attributes.items()}\n",
        "            if entity.attributes\n",
        "            else {},\n",
        "        )\n",
        "        stringified_entities.append(stringified_entity)\n",
        "    return stringified_entities\n",
        "\n",
        "\n",
        "\n",
        "entities_stringified = stringify_entity_fields(final_entities)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aBlPh7hm0a83"
      },
      "outputs": [],
      "source": [
        "def stringify_relationship_fields(relationships):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    stringified_relationships = []\n",
        "    for relationship in relationships:\n",
        "        stringified_relationship = Relationship(\n",
        "            id=str(relationship.id),\n",
        "            short_id=str(relationship.short_id),\n",
        "            source=str(relationship.source),\n",
        "            target=str(relationship.target),\n",
        "            weight=str(relationship.weight),\n",
        "            description=str(relationship.description),\n",
        "            description_embedding=str(relationship.description_embedding)\n",
        "            if relationship.description_embedding is not None\n",
        "            else \"\",\n",
        "            text_unit_ids=[\n",
        "                str(text_unit_id) for text_unit_id in relationship.text_unit_ids\n",
        "            ]\n",
        "            if relationship.text_unit_ids\n",
        "            else [],\n",
        "            rank=str(relationship.rank),\n",
        "            attributes={str(k): str(v) for k, v in relationship.attributes.items()}\n",
        "            if relationship.attributes\n",
        "            else {},\n",
        "        )\n",
        "        stringified_relationships.append(stringified_relationship)\n",
        "    return stringified_relationships\n",
        "\n",
        "relationships_stringified = stringify_relationship_fields(final_relationships)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TDzxt0M-QvHc"
      },
      "outputs": [],
      "source": [
        "#de aici"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sWz9lG_fpp24"
      },
      "outputs": [],
      "source": [
        "# Copyright (c) 2024 Microsoft Corporation.\n",
        "# Licensed under the MIT License\n",
        "\n",
        "log = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "class LocalSearchMixedContext(LocalContextBuilder):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        entities: list[Entity],\n",
        "        entity_text_embeddings: BaseVectorStore,\n",
        "        text_embedder: BaseTextEmbedding,\n",
        "        text_units: list[TextUnit] | None = None,\n",
        "        community_reports: list[CommunityReport] | None = None,\n",
        "        relationships: list[Relationship] | None = None,\n",
        "        covariates: dict[str, list[Covariate]] | None = None,\n",
        "        token_encoder: tiktoken.Encoding | None = None,\n",
        "        embedding_vectorstore_key: str = EntityVectorStoreKey.ID,\n",
        "    ):\n",
        "        if community_reports is None:\n",
        "            community_reports = []\n",
        "        if relationships is None:\n",
        "            relationships = []\n",
        "        if covariates is None:\n",
        "            covariates = {}\n",
        "        if text_units is None:\n",
        "            text_units = []\n",
        "        self.entities = {entity.id: entity for entity in entities}\n",
        "        self.community_reports = {\n",
        "            community.community_id: community for community in community_reports\n",
        "        }\n",
        "        self.text_units = {unit.id: unit for unit in text_units}\n",
        "        self.relationships = {\n",
        "            relationship.id: relationship for relationship in relationships\n",
        "        }\n",
        "        self.covariates = covariates\n",
        "        self.entity_text_embeddings = entity_text_embeddings\n",
        "        self.text_embedder = text_embedder\n",
        "        self.token_encoder = token_encoder\n",
        "        self.embedding_vectorstore_key = embedding_vectorstore_key\n",
        "\n",
        "    def filter_by_entity_keys(self, entity_keys: list[int] | list[str]):\n",
        "        \"\"\"Filter entity text embeddings by entity keys.\"\"\"\n",
        "        self.entity_text_embeddings.filter_by_id(entity_keys)\n",
        "\n",
        "    def build_context(\n",
        "        self,\n",
        "        query: str,\n",
        "        conversation_history: ConversationHistory | None = None,\n",
        "        include_entity_names: list[str] | None = None,\n",
        "        exclude_entity_names: list[str] | None = None,\n",
        "        conversation_history_max_turns: int | None = 5,\n",
        "        conversation_history_user_turns_only: bool = True,\n",
        "        max_tokens: int = 18000,\n",
        "        text_unit_prop: float = 0.5,\n",
        "        community_prop: float = 0.25,\n",
        "        top_k_mapped_entities: int = 5,\n",
        "        top_k_relationships: int = 15,\n",
        "        include_community_rank: bool = False,\n",
        "        include_entity_rank: bool = False,\n",
        "        rank_description: str = \"number of relationships\",\n",
        "        include_relationship_weight: bool = False,\n",
        "        relationship_ranking_attribute: str = \"rank\",\n",
        "        return_candidate_context: bool = False,\n",
        "        use_community_summary: bool = False,\n",
        "        min_community_rank: int = 0,\n",
        "        community_context_name: str = \"Reports\",\n",
        "        column_delimiter: str = \"|\",\n",
        "        **kwargs: dict[str, Any],\n",
        "    ) -> ContextBuilderResult:\n",
        "        \"\"\"\n",
        "        Build data context for local search prompt.\n",
        "\n",
        "        Build a context by combining community reports and entity/relationship/covariate tables, and text units using a predefined ratio set by summary_prop.\n",
        "        \"\"\"\n",
        "        if include_entity_names is None:\n",
        "            include_entity_names = []\n",
        "        if exclude_entity_names is None:\n",
        "            exclude_entity_names = []\n",
        "        if community_prop + text_unit_prop > 1:\n",
        "            value_error = (\n",
        "                \"The sum of community_prop and text_unit_prop should not exceed 1.\"\n",
        "            )\n",
        "            raise ValueError(value_error)\n",
        "\n",
        "        if conversation_history:\n",
        "            pre_user_questions = \"\\n\".join(\n",
        "                conversation_history.get_user_turns(conversation_history_max_turns)\n",
        "            )\n",
        "            query = f\"{query}\\n{pre_user_questions}\"\n",
        "\n",
        "        selected_entities = map_query_to_entities(\n",
        "            query=query,\n",
        "            text_embedding_vectorstore=self.entity_text_embeddings,\n",
        "            text_embedder=self.text_embedder,\n",
        "            all_entities_dict=self.entities,\n",
        "            embedding_vectorstore_key=self.embedding_vectorstore_key,\n",
        "            include_entity_names=include_entity_names,\n",
        "            exclude_entity_names=exclude_entity_names,\n",
        "            k=5,\n",
        "            oversample_scaler=2,\n",
        "        )\n",
        "\n",
        "\n",
        "        final_context = list[str]()\n",
        "        final_context_data = dict[str, pd.DataFrame]()\n",
        "\n",
        "        if conversation_history:\n",
        "\n",
        "            (\n",
        "                conversation_history_context,\n",
        "                conversation_history_context_data,\n",
        "            ) = conversation_history.build_context(\n",
        "                include_user_turns_only=conversation_history_user_turns_only,\n",
        "                max_qa_turns=conversation_history_max_turns,\n",
        "                column_delimiter=column_delimiter,\n",
        "                max_tokens=max_tokens,\n",
        "                recency_bias=False,\n",
        "            )\n",
        "            if conversation_history_context.strip() != \"\":\n",
        "                final_context.append(conversation_history_context)\n",
        "                final_context_data = conversation_history_context_data\n",
        "                max_tokens = max_tokens - num_tokens(\n",
        "                    conversation_history_context, self.token_encoder\n",
        "                )\n",
        "\n",
        "\n",
        "        community_tokens = max(int(max_tokens * community_prop), 0)\n",
        "        community_context, community_context_data = self._build_community_context(\n",
        "            selected_entities=selected_entities,\n",
        "            max_tokens=community_tokens,\n",
        "            use_community_summary=use_community_summary,\n",
        "            column_delimiter=column_delimiter,\n",
        "            include_community_rank=include_community_rank,\n",
        "            min_community_rank=min_community_rank,\n",
        "            return_candidate_context=return_candidate_context,\n",
        "            context_name=community_context_name,\n",
        "        )\n",
        "        if community_context.strip() != \"\":\n",
        "            final_context.append(community_context)\n",
        "            final_context_data = {**final_context_data, **community_context_data}\n",
        "\n",
        "\n",
        "        local_prop = 1 - community_prop - text_unit_prop\n",
        "        local_tokens = max(int(max_tokens * local_prop), 0)\n",
        "        local_context, local_context_data = self._build_local_context(\n",
        "            selected_entities=selected_entities,\n",
        "            max_tokens=local_tokens,\n",
        "            include_entity_rank=include_entity_rank,\n",
        "            rank_description=rank_description,\n",
        "            include_relationship_weight=include_relationship_weight,\n",
        "            top_k_relationships=top_k_relationships,\n",
        "            relationship_ranking_attribute=relationship_ranking_attribute,\n",
        "            return_candidate_context=return_candidate_context,\n",
        "            column_delimiter=column_delimiter,\n",
        "        )\n",
        "        if local_context.strip() != \"\":\n",
        "            final_context.append(str(local_context))\n",
        "            final_context_data = {**final_context_data, **local_context_data}\n",
        "\n",
        "        text_unit_tokens = max(int(max_tokens * text_unit_prop), 0)\n",
        "        text_unit_context, text_unit_context_data = self._build_text_unit_context(\n",
        "            selected_entities=selected_entities,\n",
        "            max_tokens=text_unit_tokens,\n",
        "            return_candidate_context=return_candidate_context,\n",
        "        )\n",
        "\n",
        "        if text_unit_context.strip() != \"\":\n",
        "            final_context.append(text_unit_context)\n",
        "            final_context_data = {**final_context_data, **text_unit_context_data}\n",
        "\n",
        "        return ContextBuilderResult(\n",
        "            context_chunks=\"\\n\\n\".join(final_context),\n",
        "            context_records=final_context_data,\n",
        "        )\n",
        "\n",
        "    def _build_community_context(\n",
        "        self,\n",
        "        selected_entities: list[Entity],\n",
        "        max_tokens: int = 4000,\n",
        "        use_community_summary: bool = False,\n",
        "        column_delimiter: str = \"|\",\n",
        "        include_community_rank: bool = False,\n",
        "        min_community_rank: int = 0,\n",
        "        return_candidate_context: bool = False,\n",
        "        context_name: str = \"Reports\",\n",
        "    ) -> tuple[str, dict[str, pd.DataFrame]]:\n",
        "        \"\"\"Add community data to the context window until it hits the max_tokens limit.\"\"\"\n",
        "        if len(selected_entities) == 0 or len(self.community_reports) == 0:\n",
        "            return (\"\", {context_name.lower(): pd.DataFrame()})\n",
        "\n",
        "        community_matches = {}\n",
        "        for entity in selected_entities:\n",
        "\n",
        "            if entity.community_ids:\n",
        "                for community_id in entity.community_ids:\n",
        "                    community_matches[community_id] = (\n",
        "                        community_matches.get(community_id, 0) + 1\n",
        "                    )\n",
        "        selected_communities = [\n",
        "            self.community_reports[community_id]\n",
        "            for community_id in community_matches\n",
        "            if community_id in self.community_reports\n",
        "        ]\n",
        "        for community in selected_communities:\n",
        "            if community.attributes is None:\n",
        "                community.attributes = {}\n",
        "            community.attributes[\"matches\"] = community_matches[community.community_id]\n",
        "        selected_communities.sort(\n",
        "            key=lambda x: (x.attributes[\"matches\"], x.rank),\n",
        "            reverse=True,\n",
        "        )\n",
        "        for community in selected_communities:\n",
        "            del community.attributes[\"matches\"]\n",
        "\n",
        "        context_text, context_data = build_community_context(\n",
        "            community_reports=selected_communities,\n",
        "            token_encoder=self.token_encoder,\n",
        "            use_community_summary=use_community_summary,\n",
        "            column_delimiter=column_delimiter,\n",
        "            shuffle_data=False,\n",
        "            include_community_rank=include_community_rank,\n",
        "            min_community_rank=min_community_rank,\n",
        "            max_tokens=max_tokens,\n",
        "            single_batch=True,\n",
        "            context_name=context_name,\n",
        "        )\n",
        "        if isinstance(context_text, list) and len(context_text) > 0:\n",
        "            context_text = \"\\n\\n\".join(context_text)\n",
        "\n",
        "        if return_candidate_context:\n",
        "            candidate_context_data = get_candidate_communities(\n",
        "                selected_entities=selected_entities,\n",
        "                community_reports=list(self.community_reports.values()),\n",
        "                use_community_summary=use_community_summary,\n",
        "                include_community_rank=include_community_rank,\n",
        "            )\n",
        "            context_key = context_name.lower()\n",
        "            if context_key not in context_data:\n",
        "                context_data[context_key] = candidate_context_data\n",
        "                context_data[context_key][\"in_context\"] = False\n",
        "            else:\n",
        "                if (\n",
        "                    \"id\" in candidate_context_data.columns\n",
        "                    and \"id\" in context_data[context_key].columns\n",
        "                ):\n",
        "                    candidate_context_data[\"in_context\"] = candidate_context_data[\n",
        "                        \"id\"\n",
        "                    ].isin(\n",
        "                        context_data[context_key][\"id\"]\n",
        "                    )\n",
        "                    context_data[context_key] = candidate_context_data\n",
        "                else:\n",
        "                    context_data[context_key][\"in_context\"] = True\n",
        "        return (str(context_text), context_data)\n",
        "\n",
        "    def _build_text_unit_context(\n",
        "        self,\n",
        "        selected_entities: list[Entity],\n",
        "        max_tokens: int = 18000,\n",
        "        return_candidate_context: bool = False,\n",
        "        column_delimiter: str = \"|\",\n",
        "        context_name: str = \"Sources\",\n",
        "    ) -> tuple[str, dict[str, pd.DataFrame]]:\n",
        "        \"\"\"Rank matching text units and add them to the context window until it hits the max_tokens limit.\"\"\"\n",
        "        if not selected_entities or not self.text_units:\n",
        "            return (\"\", {context_name.lower(): pd.DataFrame()})\n",
        "        selected_text_units = []\n",
        "        text_unit_ids_set = set()\n",
        "\n",
        "        unit_info_list = []\n",
        "        relationship_values = list(self.relationships.values())\n",
        "        for index, entity in enumerate(selected_entities):\n",
        "            entity_relationships = [\n",
        "                rel\n",
        "                for rel in relationship_values\n",
        "                if rel.source == entity.title or rel.target == entity.title\n",
        "            ]\n",
        "\n",
        "            for text_id in entity.text_unit_ids or []:\n",
        "                if text_id not in text_unit_ids_set and text_id in self.text_units:\n",
        "                    selected_unit = deepcopy(self.text_units[text_id])\n",
        "                    num_relationships = count_relationships(\n",
        "                        entity_relationships, selected_unit\n",
        "                    )\n",
        "                    text_unit_ids_set.add(text_id)\n",
        "                    unit_info_list.append((selected_unit, index, num_relationships))\n",
        "\n",
        "\n",
        "        unit_info_list.sort(key=lambda x: (x[1], -x[2]))\n",
        "\n",
        "        selected_text_units = [unit[0] for unit in unit_info_list]\n",
        "\n",
        "        context_text, context_data = build_text_unit_context(\n",
        "            text_units=selected_text_units,\n",
        "            token_encoder=self.token_encoder,\n",
        "            max_tokens=max_tokens,\n",
        "            shuffle_data=False,\n",
        "            context_name=context_name,\n",
        "            column_delimiter=column_delimiter,\n",
        "        )\n",
        "\n",
        "        if return_candidate_context:\n",
        "            candidate_context_data = get_candidate_text_units(\n",
        "                selected_entities=selected_entities,\n",
        "                text_units=list(self.text_units.values()),\n",
        "            )\n",
        "            context_key = context_name.lower()\n",
        "            if context_key not in context_data:\n",
        "                candidate_context_data[\"in_context\"] = False\n",
        "                context_data[context_key] = candidate_context_data\n",
        "            else:\n",
        "                if (\n",
        "                    \"id\" in candidate_context_data.columns\n",
        "                    and \"id\" in context_data[context_key].columns\n",
        "                ):\n",
        "                    candidate_context_data[\"in_context\"] = candidate_context_data[\n",
        "                        \"id\"\n",
        "                    ].isin(context_data[context_key][\"id\"])\n",
        "                    context_data[context_key] = candidate_context_data\n",
        "                else:\n",
        "                    context_data[context_key][\"in_context\"] = True\n",
        "\n",
        "        return (str(context_text), context_data)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def _build_local_context(\n",
        "        self,\n",
        "        selected_entities: list[Entity],\n",
        "        max_tokens: int = 8000,\n",
        "        include_entity_rank: bool = False,\n",
        "        rank_description: str = \"relationship count\",\n",
        "        include_relationship_weight: bool = False,\n",
        "        top_k_relationships: int = 15,\n",
        "        relationship_ranking_attribute: str = \"rank\",\n",
        "        return_candidate_context: bool = False,\n",
        "        column_delimiter: str = \"|\",\n",
        "    ) -> tuple[str, dict[str, pd.DataFrame]]:\n",
        "        \"\"\"Build data context for local search prompt combining entity/relationship/covariate tables.\"\"\"\n",
        "\n",
        "        entity_context, entity_context_data = build_entity_context(\n",
        "            selected_entities=selected_entities,\n",
        "            token_encoder=self.token_encoder,\n",
        "            max_tokens=max_tokens,\n",
        "            column_delimiter=column_delimiter,\n",
        "            include_entity_rank=include_entity_rank,\n",
        "            rank_description=rank_description,\n",
        "            context_name=\"Entities\",\n",
        "        )\n",
        "        entity_tokens = num_tokens(entity_context, self.token_encoder)\n",
        "\n",
        "        added_entities = []\n",
        "        final_context = []\n",
        "        final_context_data = {}\n",
        "\n",
        "        for entity in selected_entities:\n",
        "            current_context = []\n",
        "            current_context_data = {}\n",
        "            added_entities.append(entity)\n",
        "\n",
        "\n",
        "            (\n",
        "                relationship_context,\n",
        "                relationship_context_data,\n",
        "            ) = build_relationship_context(\n",
        "                selected_entities=added_entities,\n",
        "                relationships=list(self.relationships.values()),\n",
        "                token_encoder=self.token_encoder,\n",
        "                max_tokens=max_tokens,\n",
        "                column_delimiter=column_delimiter,\n",
        "                top_k_relationships=top_k_relationships,\n",
        "                include_relationship_weight=include_relationship_weight,\n",
        "                relationship_ranking_attribute=relationship_ranking_attribute,\n",
        "                context_name=\"Relationships\",\n",
        "            )\n",
        "\n",
        "            current_context.append(relationship_context)\n",
        "            current_context_data[\"relationships\"] = relationship_context_data\n",
        "            total_tokens = entity_tokens + num_tokens(\n",
        "                relationship_context, self.token_encoder\n",
        "            )\n",
        "\n",
        "            for covariate in self.covariates:\n",
        "                covariate_context, covariate_context_data = build_covariates_context(\n",
        "                    selected_entities=added_entities,\n",
        "                    covariates=self.covariates[covariate],\n",
        "                    token_encoder=self.token_encoder,\n",
        "                    max_tokens=max_tokens,\n",
        "                    column_delimiter=column_delimiter,\n",
        "                    context_name=covariate,\n",
        "                )\n",
        "                total_tokens += num_tokens(covariate_context, self.token_encoder)\n",
        "                current_context.append(covariate_context)\n",
        "                current_context_data[covariate.lower()] = covariate_context_data\n",
        "            final_context = current_context\n",
        "            final_context_data = current_context_data\n",
        "\n",
        "\n",
        "        final_context_text = entity_context + \"\\n\\n\" + \"\\n\\n\".join(final_context)\n",
        "        final_context_data[\"entities\"] = entity_context_data\n",
        "\n",
        "        if return_candidate_context:\n",
        "\n",
        "            candidate_context_data = get_candidate_context(\n",
        "                selected_entities=selected_entities,\n",
        "                entities=list(self.entities.values()),\n",
        "                relationships=list(self.relationships.values()),\n",
        "                covariates=self.covariates,\n",
        "                include_entity_rank=include_entity_rank,\n",
        "                entity_rank_description=rank_description,\n",
        "                include_relationship_weight=include_relationship_weight,\n",
        "            )\n",
        "            for key in candidate_context_data:\n",
        "                candidate_df = candidate_context_data[key]\n",
        "                if key not in final_context_data:\n",
        "                    final_context_data[key] = candidate_df\n",
        "                    final_context_data[key][\"in_context\"] = False\n",
        "                else:\n",
        "                    in_context_df = final_context_data[key]\n",
        "\n",
        "                    if \"id\" in in_context_df.columns and \"id\" in candidate_df.columns:\n",
        "                        candidate_df[\"in_context\"] = candidate_df[\n",
        "                            \"id\"\n",
        "                        ].isin(\n",
        "                            in_context_df[\"id\"]\n",
        "                        )\n",
        "                        final_context_data[key] = candidate_df\n",
        "                    else:\n",
        "                        final_context_data[key][\"in_context\"] = True\n",
        "        else:\n",
        "            for key in final_context_data:\n",
        "                final_context_data[key][\"in_context\"] = True\n",
        "        return (final_context_text, final_context_data)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MYpslmD-uxPS"
      },
      "outputs": [],
      "source": [
        "\n",
        "context_builder = LocalSearchMixedContext(\n",
        "    community_reports=com_report_stringified,\n",
        "    text_units=None,\n",
        "    entities=entities_stringified,\n",
        "    relationships=relationships_stringified,\n",
        "    covariates=None,\n",
        "    entity_text_embeddings=description_embedding_store,\n",
        "    embedding_vectorstore_key=EntityVectorStoreKey.TITLE,\n",
        "    text_embedder=text_embedder,\n",
        "    token_encoder=token_encoder,\n",
        "\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Copyright (c) 2024 Microsoft Corporation.\n",
        "# Licensed under the MIT License\n",
        "\"\"\"LocalSearch implementation with customizable search prompt.\"\"\"\n",
        "\n",
        "import logging\n",
        "import time\n",
        "from collections.abc import AsyncGenerator\n",
        "from typing import Any, Optional\n",
        "\n",
        "import tiktoken\n",
        "\n",
        "from graphrag.prompts.query.local_search_system_prompt import LOCAL_SEARCH_SYSTEM_PROMPT\n",
        "from graphrag.query.context_builder.builders import LocalContextBuilder\n",
        "from graphrag.query.context_builder.conversation_history import ConversationHistory\n",
        "from graphrag.query.llm.base import BaseLLM, BaseLLMCallback\n",
        "from graphrag.query.llm.text_utils import num_tokens\n",
        "from graphrag.query.structured_search.base import BaseSearch, SearchResult\n",
        "\n",
        "DEFAULT_LLM_PARAMS = {\n",
        "    \"max_tokens\": 1500,\n",
        "    \"temperature\": 0.0,\n",
        "}\n",
        "\n",
        "log = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "class LocalSearchForGPT(BaseSearch[LocalContextBuilder]):\n",
        "    \"\"\"Search orchestration for local search mode with external search prompt support.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        llm: BaseLLM,\n",
        "        context_builder: LocalContextBuilder,\n",
        "        token_encoder: Optional[tiktoken.Encoding] = None,\n",
        "        system_prompt: Optional[str] = None,\n",
        "        response_type: str = \"triples\",\n",
        "        callbacks: Optional[list[BaseLLMCallback]] = None,\n",
        "        llm_params: dict[str, Any] = DEFAULT_LLM_PARAMS,\n",
        "        context_builder_params: Optional[dict] = None,\n",
        "    ):\n",
        "        super().__init__(\n",
        "            llm=llm,\n",
        "            context_builder=context_builder,\n",
        "            token_encoder=token_encoder,\n",
        "            llm_params=llm_params,\n",
        "            context_builder_params=context_builder_params or {},\n",
        "        )\n",
        "        self.system_prompt = system_prompt or LOCAL_SEARCH_SYSTEM_PROMPT\n",
        "        self.callbacks = callbacks\n",
        "        self.response_type = response_type\n",
        "\n",
        "    async def astream_search(\n",
        "        self,\n",
        "        query: str,\n",
        "        conversation_history: Optional[ConversationHistory] = None,\n",
        "        search_prompt: Optional[str] = None,\n",
        "    ) -> AsyncGenerator[str, None]:\n",
        "        \"\"\"Asynchronous generator for streaming RDF triples with customizable search prompt.\"\"\"\n",
        "        start_time = time.time()\n",
        "\n",
        "        context_result = self.context_builder.build_context(\n",
        "            query=query,\n",
        "            conversation_history=conversation_history,\n",
        "            **self.context_builder_params,\n",
        "        )\n",
        "\n",
        "        context_data = context_result.context_chunks\n",
        "\n",
        "\n",
        "        search_prompt = search_prompt or (\n",
        "            f\"### Context ###\\n\"\n",
        "            f\"{context_data}\\n\\n\"\n",
        "            f\"### Query ###\\n\"\n",
        "            f\"{query}\\n\\n\"\n",
        "            f\"### Instructions ###\\n\"\n",
        "            f\"Extract RDF triples that **directly match** the query. \"\n",
        "            f\"Each triple must follow the format: `<subject> <predicate> <object> .` \"\n",
        "            f\"Do not infer additional triples beyond what is explicitly stated.\"\n",
        "        )\n",
        "\n",
        "        search_messages = [\n",
        "            {\"role\": \"system\", \"content\": \"You are an assistant skilled in generating RDF triples.\"},\n",
        "            {\"role\": \"user\", \"content\": search_prompt},\n",
        "        ]\n",
        "\n",
        "        yield context_result.context_records\n",
        "        async for response in self.llm.astream_generate(\n",
        "            messages=search_messages,\n",
        "            callbacks=self.callbacks,\n",
        "            **self.llm_params,\n",
        "        ):\n",
        "            yield response\n",
        "\n",
        "    def search(\n",
        "        self,\n",
        "        query: str,\n",
        "        conversation_history: Optional[ConversationHistory] = None,\n",
        "        search_prompt: Optional[str] = None,\n",
        "        **kwargs,\n",
        "    ) -> SearchResult:\n",
        "        \"\"\"Synchronous search function with customizable search prompt.\"\"\"\n",
        "        start_time = time.time()\n",
        "        llm_calls, prompt_tokens, output_tokens = {}, {}, {}\n",
        "\n",
        "        context_result = self.context_builder.build_context(\n",
        "            query=query,\n",
        "            conversation_history=conversation_history,\n",
        "            **kwargs,\n",
        "            **self.context_builder_params,\n",
        "        )\n",
        "\n",
        "        context_data = context_result.context_chunks\n",
        "\n",
        "\n",
        "        search_prompt = search_prompt or (\n",
        "            f\"### Context ###\\n\"\n",
        "            f\"{context_data}\\n\\n\"\n",
        "            f\"### Query ###\\n\"\n",
        "            f\"{query}\\n\\n\"\n",
        "            f\"### Instructions ###\\n\"\n",
        "            f\"Extract RDF triples that **directly match** the query. \"\n",
        "            f\"Each triple must follow the format: `<subject> <predicate> <object> .` \"\n",
        "            f\"Do not infer additional triples beyond what is explicitly stated.\"\n",
        "        )\n",
        "\n",
        "        search_messages = [\n",
        "            {\"role\": \"system\", \"content\": \"You are an assistant skilled in generating RDF triples.\"},\n",
        "            {\"role\": \"user\", \"content\": search_prompt},\n",
        "        ]\n",
        "\n",
        "        try:\n",
        "            print(context_data)\n",
        "            response = self.llm.generate(\n",
        "                messages=search_messages,\n",
        "                streaming=False,\n",
        "                callbacks=self.callbacks,\n",
        "                **self.llm_params,\n",
        "            )\n",
        "\n",
        "            llm_calls[\"response\"] = 1\n",
        "            prompt_tokens[\"response\"] = num_tokens(search_prompt, self.token_encoder)\n",
        "            output_tokens[\"response\"] = num_tokens(response, self.token_encoder)\n",
        "\n",
        "            return SearchResult(\n",
        "                response=response,\n",
        "                context_data=context_result.context_records,\n",
        "                context_text=context_result.context_chunks,\n",
        "                completion_time=time.time() - start_time,\n",
        "                llm_calls=sum(llm_calls.values()),\n",
        "                prompt_tokens=sum(prompt_tokens.values()),\n",
        "                output_tokens=sum(output_tokens.values()),\n",
        "                llm_calls_categories=llm_calls,\n",
        "                prompt_tokens_categories=prompt_tokens,\n",
        "                output_tokens_categories=output_tokens,\n",
        "            )\n",
        "\n",
        "        except Exception as e:\n",
        "            log.exception(\"Exception in search: %s\", e)\n",
        "            return SearchResult(\n",
        "                response=\"\",\n",
        "                context_data=context_result.context_records,\n",
        "                context_text=context_result.context_chunks,\n",
        "                completion_time=time.time() - start_time,\n",
        "                llm_calls=1,\n",
        "                prompt_tokens=num_tokens(search_prompt, self.token_encoder),\n",
        "                output_tokens=0,\n",
        "            )\n",
        "\n",
        "    async def asearch(\n",
        "        self,\n",
        "        query: str,\n",
        "        conversation_history: Optional[ConversationHistory] = None,\n",
        "        search_prompt: Optional[str] = None,\n",
        "        **kwargs,\n",
        "    ) -> SearchResult:\n",
        "        \"\"\"Asynchronous search function with customizable search prompt.\"\"\"\n",
        "        start_time = time.time()\n",
        "        llm_calls, prompt_tokens, output_tokens = {}, {}, {}\n",
        "\n",
        "        context_result = context_builder.build_context(query=query)\n",
        "        context_data = context_result.context_chunks\n",
        "\n",
        "\n",
        "        search_prompt = (\n",
        "\n",
        "            f\"{search_prompt or ''}\"\n",
        "                 f\"Query: \"\n",
        "            f\"{query}\\n\\n\"\n",
        "            f\"Context:\\n\"\n",
        "            f\"{context_data}\\n\\n\"\n",
        "        )\n",
        "\n",
        "        search_prompt = search_prompt or \"\"\n",
        "\n",
        "        search_messages = [\n",
        "            {\"role\": \"system\", \"content\": \"You are an assistant skilled in generating RDF triples.\"},\n",
        "            {\"role\": \"user\", \"content\": search_prompt},\n",
        "        ]\n",
        "\n",
        "        try:\n",
        "            response = await self.llm.agenerate(\n",
        "                messages=search_messages,\n",
        "                streaming=False,\n",
        "                callbacks=self.callbacks,\n",
        "                **self.llm_params,\n",
        "            )\n",
        "            print(response)\n",
        "\n",
        "            llm_calls[\"response\"] = 1\n",
        "            prompt_tokens[\"response\"] = num_tokens(search_prompt, self.token_encoder)\n",
        "            output_tokens[\"response\"] = num_tokens(response, self.token_encoder)\n",
        "\n",
        "            return SearchResult(\n",
        "                response=response,\n",
        "                context_data=context_result.context_records,\n",
        "                context_text=context_result.context_chunks,\n",
        "                completion_time=time.time() - start_time,\n",
        "                llm_calls=sum(llm_calls.values()),\n",
        "                prompt_tokens=sum(prompt_tokens.values()),\n",
        "                output_tokens=sum(output_tokens.values()),\n",
        "                llm_calls_categories=llm_calls,\n",
        "                prompt_tokens_categories=prompt_tokens,\n",
        "                output_tokens_categories=output_tokens,\n",
        "            )\n",
        "\n",
        "        except Exception as e:\n",
        "            log.exception(\"Exception in asearch: %s\", e)\n",
        "            return SearchResult(\n",
        "                response=\"\",\n",
        "                context_data=context_result.context_records,\n",
        "                context_text=context_result.context_chunks,\n",
        "                completion_time=time.time() - start_time,\n",
        "                llm_calls=1,\n",
        "                prompt_tokens=num_tokens(search_prompt, self.token_encoder),\n",
        "                output_tokens=0,\n",
        "            )\n"
      ],
      "metadata": {
        "id": "gvQCtAdP3z8U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n0iKO2zt0XWL"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "def extract_triples(data):\n",
        "    triples = []\n",
        "    if not data or 'results' not in data or 'bindings' not in data['results']:\n",
        "        return triples\n",
        "\n",
        "    for binding in data['results']['bindings']:\n",
        "        sub = binding['subject']['value'].split(\"/\")[-1]\n",
        "        pred = binding['predicate']['value'].split(\"/\")[-1]\n",
        "        obj = binding['object']['value'].split(\"/\")[-1]\n",
        "        triples.append(f\"{sub} {pred} {obj}\")\n",
        "\n",
        "    return triples\n",
        "\n",
        "\n",
        "def extract_array_from_string(text):\n",
        "    if not text or not isinstance(text, str):\n",
        "        return []\n",
        "\n",
        "    match = re.search(r'\\[.*\\]', text, re.DOTALL)\n",
        "    if match:\n",
        "        array_str = match.group(0)\n",
        "        try:\n",
        "            triples_list = ast.literal_eval(array_str)\n",
        "            if isinstance(triples_list, list):\n",
        "                return [clean_triple(triple) for triple in triples_list]\n",
        "        except (SyntaxError, ValueError):\n",
        "            pass\n",
        "\n",
        "\n",
        "    text = re.sub(r'\\d+\\.\\s*', '', text)\n",
        "    text = re.sub(r'^-\\s*', '', text, flags=re.MULTILINE)\n",
        "    text = text.strip()\n",
        "    triples = text.split(\"\\n\")\n",
        "\n",
        "    return [clean_triple(triple) for triple in triples if len(triple.split()) >= 3]\n",
        "\n",
        "\n",
        "def clean_triple(triple):\n",
        "    triple = triple.strip()\n",
        "    triple = re.sub(r'[^a-zA-Z0-9_ ]', '', triple)\n",
        "    return \" \".join(triple.split())\n",
        "\n",
        "\n",
        "def extract_triples_from_turtle(response_text):\n",
        "    if not response_text or not isinstance(response_text, str):\n",
        "        return []\n",
        "\n",
        "\n",
        "    response_text = re.sub(r'```turtle\\s*|\\s*```', '', response_text, flags=re.DOTALL).strip()\n",
        "\n",
        "\n",
        "    triples = []\n",
        "    for line in response_text.split(\"\\n\"):\n",
        "        clean_line = line.strip().rstrip('.')\n",
        "        clean_line = re.sub(r'[`]', '', clean_line)\n",
        "        clean_line = \" \".join(clean_line.split())\n",
        "        words = clean_line.split()\n",
        "\n",
        "        if len(words) == 3:\n",
        "            triples.append(clean_line)\n",
        "\n",
        "    return triples\n",
        "\n",
        "def find_common_and_extra_elements(benchmark_triples, model_response_triples):\n",
        "\n",
        "    normalized_benchmark = set(benchmark_triples)\n",
        "    normalized_model_response = set(model_response_triples)\n",
        "\n",
        "    print(\"\\n=== Benchmark Triples (Normalized) ===\")\n",
        "    print(normalized_benchmark)\n",
        "    print(\"\\n=== LLM Triples (Normalized) ===\")\n",
        "    print(normalized_model_response)\n",
        "\n",
        "\n",
        "    common_elements = list(normalized_benchmark & normalized_model_response)\n",
        "    extra_elements = list(normalized_model_response - normalized_benchmark)\n",
        "    precision = len(common_elements) / len(normalized_model_response) if len(normalized_model_response) > 0 else 0\n",
        "    recall = len(common_elements) / len(normalized_benchmark) if len(normalized_benchmark) > 0 else 0\n",
        "    f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "    return common_elements, extra_elements, precision, recall, f1\n",
        "\n",
        "def extract_triples_formatted(response_text):\n",
        "    if not response_text or not isinstance(response_text, str):\n",
        "        return []\n",
        "\n",
        "    response_text = re.sub(r'^.*?Extracted RDF Triples:\\s*', '', response_text, flags=re.DOTALL)\n",
        "    response_text = re.sub(r'^\\s*\\d+\\.\\s*', '', response_text, flags=re.MULTILINE)\n",
        "    response_text = re.sub(r'(?<=\\w)-(?=\\w)', '', response_text)\n",
        "    response_text = re.sub(r'[\\[\\]\\(\\)\\.\\,\\\\]', ' ', response_text)\n",
        "    response_text = re.sub(r'\\s+', ' ', response_text).strip()\n",
        "    pattern = r'\\b([a-zA-Z0-9_]+)\\s+([a-zA-Z0-9_]+)\\s+([a-zA-Z0-9_]+)\\b'\n",
        "    matches = re.findall(pattern, response_text)\n",
        "\n",
        "    return [f\"{s} {p} {o}\" for s, p, o in matches if s and p and o]"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9MtgOAL854Uy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt1f= \"\"\"You are a Knowledge Graph Expert. You will be provided with a community summary report and a query, both written in natural language. Your task is to extract relevant knowledge in the form of RDF triples to accurately answer the query based on the information contained in the community reports.\n",
        "Provide the extracted RDF triples in the format: 'subject predicate object' as a structured list without additional explanations or descriptions, avoid numbering the elements and do not include additional special charactes or additional text. If no matching triples are found, return 'No matching triples found.'\"\"\""
      ],
      "metadata": {
        "id": "CNE1VZK45dgV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt2f=\"\"\"You are a Knowledge Graph Expert. You will be provided with a community summary report and a query, both written in natural language. Your task is to extract relevant knowledge in the form of RDF triples to accurately answer the query based on the information contained in the community reports.\n",
        "If the query specifies the predicate, return only those triples that match that exact predicate.\n",
        "Provide the extracted RDF triples in the format: 'subject predicate object' as a structured list without additional explanations or descriptions, avoid numbering the elements and do not include additional special characters or additional text. If no matching triples are found, return 'No matching triples found.\"\"\""
      ],
      "metadata": {
        "id": "T5LjA__f5kU1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt3f=\"\"\"\n",
        "You are an expert in RDF triple extraction from structured data. Your task is to extract subject-predicate-object triples from a community summary report based on a given query.\n",
        "\n",
        "You are a Knowledge Graph Expert. You will be provided with a community summary report and a query, both written in natural language. Your task is to extract relevant knowledge in the form of RDF triples to accurately answer the query based on the information contained in the community reports.\n",
        "The report consists of two sections:\n",
        "\n",
        "- Entities Section: Contains entities along with a semicolon-separated list of predicate-object pairs.\n",
        "- Relationships Section: Contains relationships between entities, specifying a source entity, target entity, and the predicate that connects them.\n",
        "\n",
        "Extraction Tasks\n",
        "\n",
        "1. Identify key elements in the query:\n",
        "   - Determine if the query specifies a subject, object, or predicate.\n",
        "   - If an entity is missing, infer it based on available predicates.\n",
        "\n",
        "2. Search the Entities Section:\n",
        "   - If an entity is mentioned, find relevant triples where it appears as a subject or object.\n",
        "   - Match predicates exactly when provided.\n",
        "\n",
        "3. Search the Relationships Section:\n",
        "   - Locate relationships that involve the mentioned entities.\n",
        "   - Extract all predicates that connect two entities when no specific predicate is given.\n",
        "\n",
        "4. Handle queries with missing information:\n",
        "   - If only a predicate is provided, retrieve all subjects and objects associated with it.\n",
        "   - If only entities are given, extract all relationships between them.\n",
        "\n",
        "5. Format the extracted triples correctly:\n",
        "   - Each triple should follow the format:\n",
        "     <subject> <predicate> <object> .\n",
        "\n",
        "6. Ensure output constraints:\n",
        "   - If no matching triples are found, return \"No matching triples found.\"\n",
        "   -Provide the extracted RDF triples as a structured list without additional explanations or descriptions.\n",
        "   -Each triple should be in the format: 'subject predicate object'. Do not separate into different sections, just list all triples in a single combined list.\n",
        "   -Avoid numbering the elements.\n",
        "   -Do not add special characters.\n",
        "   -Do not generate additional textonly output the extracted triples.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "zntc4wqv5nPt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "prompt4f = \"\"\"You are an expert in RDF triple extraction from structured data. Your task is to extract subject-predicate-object triples from a community summary report based on a given query.\n",
        "\n",
        "You are a Knowledge Graph Expert. You will be provided with a community summary report and a query, both written in natural language. Your task is to extract relevant knowledge in the form of RDF triples to accurately answer the query based on the information contained in the community reports.\n",
        "The report consists of two sections:\n",
        "\n",
        "- Entities Section: Contains entities along with a semicolon-separated list of predicate-object pairs.\n",
        "- Relationships Section: Contains relationships between entities, specifying a source entity, target entity, and the predicate that connects them.\n",
        "\n",
        "Extraction Tasks\n",
        "\n",
        "1. Identify key elements in the query:\n",
        "   - Determine if the query specifies a subject, object, or predicate.\n",
        "   - If an entity is missing, infer it based on available predicates.\n",
        "\n",
        "2. Search the Entities Section:\n",
        "   - If an entity is mentioned, find relevant triples where it appears as a subject or object.\n",
        "   - Match predicates exactly when provided.\n",
        "\n",
        "3. Search the Relationships Section:\n",
        "   - Locate relationships that involve the mentioned entities.\n",
        "   - Extract all predicates that connect two entities when no specific predicate is given.\n",
        "\n",
        "4. Handle queries with missing information:\n",
        "   - If only a predicate is provided, retrieve all subjects and objects associated with it.\n",
        "   - If only entities are given, extract all relationships between them.\n",
        "\n",
        "5. Format the extracted triples correctly:\n",
        "   - Each triple should follow the format:\n",
        "     <subject> <predicate> <object> .\n",
        "\n",
        "6. Ensure output constraints:\n",
        "   - If no matching triples are found, return \"No matching triples found.\"\n",
        "   -Provide the extracted RDF triples as a structured list without additional explanations or descriptions.\n",
        "   -Each triple should be in the format: 'subject predicate object'. Do not separate into different sections, just list all triples in a single combined list.\n",
        "   -Avoid numbering the elements.\n",
        "   -Do not add special characters.\n",
        "   -Do not generate additional textonly output the extracted triples.\n",
        "\n",
        "Example:\n",
        "Report: Human - eats fish, herbs; drinks water, juice;\n",
        "        eats - source: human target: fish, herbs\n",
        "             - source: animal target:human\n",
        "        drinks - source: human target: water, juice\n",
        "\n",
        "Example 1 Reading the target for a known relationship with a known entity\n",
        "Input Query :\n",
        "\"What does a human eat?\"\n",
        "\n",
        "Output:\n",
        "human eats fish.\n",
        "human eats herbs.\n",
        "\n",
        "Example 2\n",
        "Input Query: Reading the entity(ies) related to a known node through a known relationship\n",
        "\"Retrieve all entities that a human eats as well as all entities that eat a human?\"\n",
        "\n",
        "Output\n",
        "human eats fish.\n",
        "human eats herbs.\n",
        "animal eats human.\n",
        "\n",
        "Example 3 Reading the relationship(s) that directly connect two known entities\n",
        "Input Query:\n",
        "\"What predicates link 'human' and 'fish'?\"\n",
        "\n",
        "Output:\n",
        "human eats fish.\n",
        "\n",
        "\n",
        "Example 4\n",
        "Discovering all relationships of an entity and their targets\n",
        "Input Query:\"Find the relationships and attributes for human\"\n",
        "Output:\n",
        "human eats fish.\n",
        "human eats herbs.\n",
        "human drinks water.\n",
        "human drinks juice.\n",
        "\n",
        "\n",
        "Example 5\n",
        "Return pairs of nodes connected by a relationship\n",
        "Input Query: For the predicate 'eats', return all entities that are directly connected by it, even if they are subject or object.\n",
        "Output: anmial eats human.\n",
        "        human eats fish.\n",
        "        human eats herbs.\n",
        "\n",
        "\n",
        "Example 6\n",
        "Discovering all triples that involve an entity\n",
        "Input Query:\n",
        "\"What do you know about human?\"\n",
        "\n",
        "Output:\n",
        "human eats fish.\n",
        "human eats herbs.\n",
        "human drinks water.\n",
        "human drinks juice.\n",
        "anmial eats human.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "CVrnUtaJ5t9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm1 =  ChatOpenAI(\n",
        "    api_key=api_key,\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    api_type=OpenaiApiType.OpenAI,\n",
        "    max_retries=20,\n",
        ")\n",
        "\n",
        "llm2 =  ChatOpenAI(\n",
        "    api_key=api_key,\n",
        "    model=\"gpt-4o\",\n",
        "    api_type=OpenaiApiType.OpenAI,\n",
        "    max_retries=20,\n",
        ")"
      ],
      "metadata": {
        "id": "DcXVc_CA57_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "se1 = LocalSearchForGPT(\n",
        "    llm=llm1,\n",
        "    context_builder=context_builder,\n",
        "    token_encoder=token_encoder,\n",
        "    llm_params=llm_params,\n",
        "    context_builder_params=local_context_params,\n",
        "    response_type=\"triples\",\n",
        ")\n",
        "se2 = LocalSearchForGPT(\n",
        "    llm=llm2,\n",
        "    context_builder=context_builder,\n",
        "    token_encoder=token_encoder,\n",
        "    llm_params=llm_params,\n",
        "    context_builder_params=local_context_params,\n",
        "    response_type=\"triples\",\n",
        ")"
      ],
      "metadata": {
        "id": "xXlEj_Fr6HsK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SPARQL_ENDPOINT = \"\" #insert SPARQL endpoint"
      ],
      "metadata": {
        "id": "7-a5iD5B7BLH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_triples_type_describe(sparql_output):\n",
        "    triples = []\n",
        "    lines = sparql_output.split(\"\\n\")\n",
        "\n",
        "    for line in lines:\n",
        "        match = re.findall(r'<http://graphrag.com/([^>]*)>', line)\n",
        "        if len(match) == 3:\n",
        "            triples.append(f\"{match[0]} {match[1]} {match[2]}\")\n",
        "\n",
        "    return triples\n",
        "\n",
        "async def query_sparql_type7(subject):\n",
        "    sparql = SPARQLWrapper(SPARQL_ENDPOINT)\n",
        "    sparql.setReturnFormat(JSON)\n",
        "\n",
        "    query = f\"\"\"\n",
        "    PREFIX rag: <http://graphrag.com/>\n",
        "\n",
        "    DESCRIBE rag:{subject}\n",
        "    \"\"\"\n",
        "\n",
        "    sparql.setQuery(query)\n",
        "\n",
        "    try:\n",
        "        ret = sparql.query().convert()\n",
        "        if isinstance(ret, bytes):\n",
        "            ret = ret.decode('utf-8')\n",
        "        return (extract_triples_type_describe(ret))\n",
        "    except Exception as e:\n",
        "        print(f\"Error querying SPARQL: {e}\")\n",
        "        return []\n",
        "\n",
        "\n",
        "async def compute_accuracy_type7(queries, prompt_level, local_search):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    accuracy_results = {'accuracy': [], 'precision': [], 'recall': [], 'f1_score': []}\n",
        "    response_results = {'results': []}\n",
        "\n",
        "    for query_data in queries:\n",
        "        query = query_data['query']\n",
        "        subject = query_data['subject']\n",
        "\n",
        "        ground_truth_triples = await query_sparql_type7(subject)\n",
        "        print(f\"\\nQuery: {query}:\")\n",
        "        response = await local_search.asearch(query=query, search_prompt=prompt_level)\n",
        "        response_text = response.response\n",
        "        model_response_triples = extract_triples_formatted(response_text)\n",
        "        print(model_response_triples)\n",
        "        common, extra, precision, recall, f1_score = find_common_and_extra_elements(ground_truth_triples, model_response_triples)\n",
        "        accuracy_results['precision'].append(precision)\n",
        "        accuracy_results['recall'].append(recall)\n",
        "        accuracy_results['f1_score'].append(f1_score)\n",
        "        response_results['results'].append(response_text)\n",
        "\n",
        "        print(\"Common Triples:\", common)\n",
        "        print(\"Extra Triples:\", extra)\n",
        "        print(f\"Precision: {precision:.2f}, Recall: {recall:.2f}, F1-score: {f1_score:.2f}\")\n",
        "        print(f\"Response: {response_text}\\n\")\n",
        "\n",
        "    mean_metrics = {\n",
        "        'precision': np.mean(accuracy_results['precision']) if accuracy_results['precision'] else 0,\n",
        "        'recall': np.mean(accuracy_results['recall']) if accuracy_results['recall'] else 0,\n",
        "        'f1_score': np.mean(accuracy_results['f1_score']) if accuracy_results['f1_score'] else 0,\n",
        "    }\n",
        "\n",
        "    return accuracy_results, mean_metrics, response_results\n",
        "\n",
        "\n",
        "queries_type7 = [\n",
        "    {\n",
        "        \"query\": \"What do you know about organism? Return all triples even when it is subject or object.\",\n",
        "        \"subject\": \"organism\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"What do you know about disease_or_syndrome? Return all triples even when it is subject or object.\",\n",
        "        \"subject\": \"disease_or_syndrome\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"What do you know about organ_or_tissue_function? Return all triples even when it is subject or object.\",\n",
        "        \"subject\": \"organ_or_tissue_function\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"What do you know about cell_or_molecular_dysfunction? Return all triples even when it is subject or object.\",\n",
        "        \"subject\": \"cell_or_molecular_dysfunction\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"What do you know about genetic_function? Return all triples even when it is subject or object.\",\n",
        "        \"subject\": \"genetic_function\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"What do you know about cell_function? Return all triples even when it is subject or object.\",\n",
        "        \"subject\": \"cell_function\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"What do you know about bird? Return all triples even when it is subject or object.\",\n",
        "        \"subject\": \"bird\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"What do you know about antibiotic? Return all triples even when it is subject or object.\",\n",
        "        \"subject\": \"antibiotic\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"What do you know about injury_or_poisoning? Return all triples even when it is subject or object.\",\n",
        "        \"subject\": \"injury_or_poisoning\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"What do you know about eicosanoid? Return all triples even when it is subject or object.\",\n",
        "        \"subject\": \"eicosanoid\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"What do you know about chemical_viewed_structurally? Return all triples even when it is subject or object.\",\n",
        "        \"subject\": \"chemical_viewed_structurally\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"What do you know about vertebrate? Return all triples even when it is subject or object.\",\n",
        "        \"subject\": \"vertebrate\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"What do you know about social_behavior? Return all triples even when it is subject or object.\",\n",
        "        \"subject\": \"social_behavior\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"What do you know about body_substance? Return all triples even when it is subject or object.\",\n",
        "        \"subject\": \"body_substance\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"What do you know about diagnostic_procedure? Return all triples even when it is subject or object.\",\n",
        "        \"subject\": \"diagnostic_procedure\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"What do you know about therapeutic_or_preventive_procedure? Return all triples even when it is subject or object.\",\n",
        "        \"subject\": \"therapeutic_or_preventive_procedure\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"What do you know about amino_acid_sequence? Return all triples even when it is subject or object.\",\n",
        "        \"subject\": \"amino_acid_sequence\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"What do you know about age_group? Return all triples even when it is subject or object.\",\n",
        "        \"subject\": \"age_group\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"What do you know about tissue? Return all triples even when it is subject or object.\",\n",
        "        \"subject\": \"tissue\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"What do you know about carbohydrate? Return all triples even when it is subject or object.\",\n",
        "        \"subject\": \"carbohydrate\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"What do you know about vitamin? Return all triples even when it is subject or object.\",\n",
        "        \"subject\": \"vitamin\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"What do you know about body_part_organ_or_organ_component? Return all triples even when it is subject or object.\",\n",
        "        \"subject\": \"body_part_organ_or_organ_component\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"What do you know about biomedical_occupation_or_discipline? Return all triples even when it is subject or object.\",\n",
        "        \"subject\": \"biomedical_occupation_or_discipline\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"What do you know about bacterium? Return all triples even when it is subject or object.\",\n",
        "        \"subject\": \"bacterium\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"What do you know about steroid? Return all triples even when it is subject or object.\",\n",
        "        \"subject\": \"steroid\"\n",
        "    }\n",
        "]\n"
      ],
      "metadata": {
        "id": "6eSaSC8Q6MIs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "async def query_sparql_type6(predicate):\n",
        "    \"\"\"Queries the SPARQL endpoint with a dynamic subject and predicate and extracts triples.\"\"\"\n",
        "    sparql = SPARQLWrapper(SPARQL_ENDPOINT)\n",
        "    sparql.setReturnFormat(JSON)\n",
        "    query = f\"\"\"\n",
        "      PREFIX rag: <http://graphrag.com/>\n",
        "\n",
        "      SELECT ?subject ?predicate ?object\n",
        "      WHERE {{\n",
        "          {{ ?subject rag:{predicate} ?object. }}\n",
        "          UNION\n",
        "          {{ ?object rag:{predicate} ?subject. }}\n",
        "          BIND(rag:{predicate} AS ?predicate)\n",
        "      }}\n",
        "      \"\"\"\n",
        "    sparql.setQuery(query)\n",
        "\n",
        "    try:\n",
        "        ret = sparql.queryAndConvert()\n",
        "        return extract_triples(ret)\n",
        "    except Exception as e:\n",
        "        print(f\"Error querying SPARQL: {e}\")\n",
        "        return []\n",
        "\n",
        "\n",
        "async def compute_accuracy_for_type6(queries, prompt_level, local_search):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    accuracy_results = {'accuracy': [], 'precision': [], 'recall': [], 'f1_score': []}\n",
        "    response_results = {'results': []}\n",
        "\n",
        "    for query_data in queries:\n",
        "        query = query_data['query']\n",
        "        predicate = query_data['predicate']\n",
        "\n",
        "        ground_truth_triples = await query_sparql_type6(predicate)\n",
        "        print(f\"\\nQuery: {query}:\")\n",
        "        response = await local_search.asearch(query=query, search_prompt=prompt_level)\n",
        "        response_text = response.response\n",
        "        model_response_triples = extract_triples_formatted(response_text)\n",
        "        print(model_response_triples)\n",
        "\n",
        "        common, extra, precision, recall, f1_score = find_common_and_extra_elements(ground_truth_triples, model_response_triples)\n",
        "\n",
        "        accuracy_results['precision'].append(precision)\n",
        "        accuracy_results['recall'].append(recall)\n",
        "        accuracy_results['f1_score'].append(f1_score)\n",
        "        response_results['results'].append(response_text)\n",
        "\n",
        "\n",
        "        print(\"Common Triples:\", common)\n",
        "        print(\"Extra Triples:\", extra)\n",
        "        print(f\"Precision: {precision:.2f}, Recall: {recall:.2f}, F1-score: {f1_score:.2f}\")\n",
        "        print(f\"Response: {response_text}\\n\")\n",
        "\n",
        "    mean_metrics = {\n",
        "        'precision': np.mean(accuracy_results['precision']) if accuracy_results['precision'] else 0,\n",
        "        'recall': np.mean(accuracy_results['recall']) if accuracy_results['recall'] else 0,\n",
        "        'f1_score': np.mean(accuracy_results['f1_score']) if accuracy_results['f1_score'] else 0,\n",
        "    }\n",
        "\n",
        "    return accuracy_results, mean_metrics, response_results\n",
        "\n",
        "queries_type6 = [\n",
        "    {\n",
        "        \"query\": \"For the predicate 'degree_of', return all entities that are directly connected by it, even if they are subject or object.\",\n",
        "        \"predicate\": \"degree_of\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"For the predicate 'ingredient_of', return all entities that are directly connected by it, even if they are subject or object.\",\n",
        "        \"predicate\": \"ingredient_of\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"For the predicate 'consists_of', return all entities that are directly connected by it, even if they are subject or object.\",\n",
        "        \"predicate\": \"consists_of\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"For the predicate 'exhibits', return all entities that are directly connected by it, even if they are subject or object.\",\n",
        "        \"predicate\": \"exhibits\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"For the predicate 'derivative_of', return all entities that are directly connected by it, even if they are subject or object.\",\n",
        "        \"predicate\": \"derivative_of\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"For the predicate 'developmental_form_of', return all entities that are directly connected by it, even if they are subject or object.\",\n",
        "        \"predicate\": \"developmental_form_of\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"For the predicate 'treats', return all entities that are directly connected by it, even if they are subject or object.\",\n",
        "        \"predicate\": \"treats\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"For the predicate 'issue_in', return all entities that are directly connected by it, even if they are subject or object.\",\n",
        "        \"predicate\": \"issue_in\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"For the predicate 'uses', return all entities that are directly connected by it, even if they are subject or object.\",\n",
        "        \"predicate\": \"uses\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"For the predicate 'interconnects', return all entities that are directly connected by it, even if they are subject or object.\",\n",
        "        \"predicate\": \"interconnects\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"For the predicate 'adjacent_to', return all entities that are directly connected by it, even if they are subject or object.\",\n",
        "        \"predicate\": \"adjacent_to\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"For the predicate 'indicates', return all entities that are directly connected by it, even if they are subject or object.\",\n",
        "        \"predicate\": \"indicates\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"For the predicate 'surrounds', return all entities that are directly connected by it, even if they are subject or object.\",\n",
        "        \"predicate\": \"surrounds\"\n",
        "    },\n",
        "     {\n",
        "        \"query\": \"For the predicate 'result_of', return all entities that are directly connected by it, even if they are subject or object.\",\n",
        "        \"predicate\": \"result_of\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"For the predicate 'affects', return all entities that are directly connected by it, even if they are subject or object.\",\n",
        "        \"predicate\": \"affects\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"For the predicate 'precedes', return all entities that are directly connected by it, even if they are subject or object.\",\n",
        "        \"predicate\": \"precedes\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"For the predicate 'complicates', return all entities that are directly connected by it, even if they are subject or object.\",\n",
        "        \"predicate\": \"complicates\"\n",
        "    },\n",
        "\n",
        "    {\n",
        "        \"query\": \"For the predicate 'manifestation_of', return all entities that are directly connected by it, even if they are subject or object.\",\n",
        "        \"predicate\": \"manifestation_of\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"For the predicate 'process_of', return all entities that are directly connected by it, even if they are subject or object.\",\n",
        "        \"predicate\": \"process_of\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"For the predicate 'isa', return all entities that are directly connected by it, even if they are subject or object.\",\n",
        "        \"predicate\": \"isa\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"For the predicate 'associated_with', return all entities that are directly connected by it, even if they are subject or object.\",\n",
        "        \"predicate\": \"associated_with\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"For the predicate 'interacts_with', return all entities that are directly connected by it, even if they are subject or object.\",\n",
        "        \"predicate\": \"interacts_with\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"For the predicate 'produces', return all entities that are directly connected by it, even if they are subject or object.\",\n",
        "        \"predicate\": \"produces\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"For the predicate 'occurs_in', return all entities that are directly connected by it, even if they are subject or object.\",\n",
        "        \"predicate\": \"occurs_in\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"For the predicate 'conceptually_related_to', return all entities that are directly connected by it, even if they are subject or object.\",\n",
        "        \"predicate\": \"conceptually_related_to\"\n",
        "    },\n",
        "]"
      ],
      "metadata": {
        "id": "UA4ZsNlw49Gf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FIOxCdZgvKdN"
      },
      "outputs": [],
      "source": [
        "async def query_sparql_type5(subject):\n",
        "    \"\"\"Queries the SPARQL endpoint with a dynamic subject and predicate and extracts triples.\"\"\"\n",
        "    sparql = SPARQLWrapper(SPARQL_ENDPOINT)\n",
        "    sparql.setReturnFormat(JSON)\n",
        "\n",
        "    query = f\"\"\"\n",
        "    PREFIX rag: <http://graphrag.com/>\n",
        "\n",
        "    SELECT ?subject ?predicate ?object\n",
        "    WHERE {{rag:{subject} ?predicate ?object.\n",
        "        BIND(rag:{subject} AS ?subject)\n",
        "      }}\n",
        "    \"\"\"\n",
        "    sparql.setQuery(query)\n",
        "\n",
        "    try:\n",
        "        ret = sparql.queryAndConvert()\n",
        "        return extract_triples(ret)\n",
        "    except Exception as e:\n",
        "        print(f\"Error querying SPARQL: {e}\")\n",
        "        return []\n",
        "\n",
        "\n",
        "\n",
        "async def compute_accuracy_for_query_type5(queries, prompt_level, local_search):\n",
        "    \"\"\"\n",
        "    Computes accuracy, precision, recall, and F1-score for query results.\n",
        "    \"\"\"\n",
        "    accuracy_results = {'accuracy': [], 'precision': [], 'recall': [], 'f1_score': []}\n",
        "    response_results = {'results': []}\n",
        "\n",
        "    for query_data in queries:\n",
        "        query = query_data['query']\n",
        "        subject = query_data['subject']\n",
        "\n",
        "        ground_truth_triples = await query_sparql_type5(subject)\n",
        "\n",
        "        print(f\"\\nQuery: {query}:\")\n",
        "        response = await local_search.asearch(query=query, search_prompt=prompt_level)\n",
        "        response_text = response.response\n",
        "        model_response_triples = extract_triples_formatted(response_text)\n",
        "        print(model_response_triples)\n",
        "        common, extra, precision, recall, f1_score = find_common_and_extra_elements(ground_truth_triples, model_response_triples)\n",
        "        accuracy_results['precision'].append(precision)\n",
        "        accuracy_results['recall'].append(recall)\n",
        "        accuracy_results['f1_score'].append(f1_score)\n",
        "        response_results['results'].append(response_text)\n",
        "        print(\"Common Triples:\", common)\n",
        "        print(\"Extra Triples:\", extra)\n",
        "        print(f\"Precision: {precision:.2f}, Recall: {recall:.2f}, F1-score: {f1_score:.2f}\")\n",
        "        print(f\"Response: {response_text}\\n\")\n",
        "\n",
        "    mean_metrics = {\n",
        "        'precision': np.mean(accuracy_results['precision']) if accuracy_results['precision'] else 0,\n",
        "        'recall': np.mean(accuracy_results['recall']) if accuracy_results['recall'] else 0,\n",
        "        'f1_score': np.mean(accuracy_results['f1_score']) if accuracy_results['f1_score'] else 0,\n",
        "    }\n",
        "\n",
        "    return accuracy_results, mean_metrics, response_results\n",
        "\n",
        "queries_type5 = [\n",
        "    {\n",
        "        \"query\": \"Find the relationships and attributes for 'disease_or_syndrome'.\",\n",
        "        \"subject\": \"disease_or_syndrome\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"Find the relationships and attributes for 'alga'.\",\n",
        "        \"subject\": \"alga\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"Find the relationships and attributes for 'organ_or_tissue_function'.\",\n",
        "        \"subject\": \"organ_or_tissue_function\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"Find the relationships and attributes for 'animal'.\",\n",
        "        \"subject\": \"animal\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"Find the relationships and attributes for 'reptile'.\",\n",
        "        \"subject\": \"reptile\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"Find the relationships and attributes for 'fish'.\",\n",
        "        \"subject\": \"fish\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"Find the relationships and attributes for 'vertebrate'.\",\n",
        "        \"subject\": \"vertebrate\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"Find the relationships and attributes for 'age_group'.\",\n",
        "        \"subject\": \"age_group\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"Find the relationships and attributes for 'classification'.\",\n",
        "        \"subject\": \"classification\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"Find the relationships and attributes for 'food'.\",\n",
        "        \"subject\": \"food\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"Find the relationships and attributes for 'clinical_drug'.\",\n",
        "        \"subject\": \"clinical_drug\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"Find the relationships and attributes for 'molecular_biology_research_technique'.\",\n",
        "        \"subject\": \"molecular_biology_research_technique\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"Find the relationships and attributes for 'occupation_or_discipline'.\",\n",
        "        \"subject\": \"occupation_or_discipline\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"Find the relationships and attributes for 'body_substance'.\",\n",
        "        \"subject\": \"body_substance\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"Find the relationships and attributes for 'carbohydrate'.\",\n",
        "        \"subject\": \"carbohydrate\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"Find the relationships and attributes for 'lipid'.\",\n",
        "        \"subject\": \"lipid\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"Find the relationships and attributes for 'hormone'.\",\n",
        "        \"subject\": \"hormone\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"Find the relationships and attributes for 'embryonic_structure'.\",\n",
        "        \"subject\": \"embryonic_structure\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"Find the relationships and attributes for 'rickettsia_or_chlamydia'.\",\n",
        "        \"subject\": \"rickettsia_or_chlamydia\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"Find the relationships and attributes for 'self_help_or_relief_organization'.\",\n",
        "        \"subject\": \"self_help_or_relief_organization\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"Find the relationships and attributes for 'research_device'.\",\n",
        "        \"subject\": \"research_device\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"Find the relationships and attributes for 'governmental_or_regulatory_activity'.\",\n",
        "        \"subject\": \"governmental_or_regulatory_activity\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"Find the relationships and attributes for 'behavior'.\",\n",
        "        \"subject\": \"behavior\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"Find the relationships and attributes for 'enzyme'.\",\n",
        "        \"subject\": \"enzyme\"\n",
        "    },\n",
        "\n",
        "       {\n",
        "        \"query\": \"Find the relationships and attributes for 'amino_acid_peptide_or_protein'.\",\n",
        "        \"subject\": \"amino_acid_peptide_or_protein\"\n",
        "    },\n",
        "\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "async def query_sparql_type4(predicate, obj):\n",
        "    \"\"\"Queries the SPARQL endpoint with a dynamic subject and predicate and extracts triples.\"\"\"\n",
        "    sparql = SPARQLWrapper(SPARQL_ENDPOINT)\n",
        "    sparql.setReturnFormat(JSON)\n",
        "\n",
        "    query = f\"\"\"\n",
        "    PREFIX rag: <http://graphrag.com/>\n",
        "\n",
        "    SELECT ?subject ?predicate ?object\n",
        "    WHERE {{ ?subject rag:{predicate} rag:{obj}.\n",
        "        BIND(rag:{obj} AS ?object)\n",
        "        BIND(rag:{predicate} AS ?predicate)\n",
        "      }}\n",
        "    \"\"\"\n",
        "    sparql.setQuery(query)\n",
        "\n",
        "    try:\n",
        "        ret = sparql.queryAndConvert()\n",
        "        return extract_triples(ret)\n",
        "    except Exception as e:\n",
        "        print(f\"Error querying SPARQL: {e}\")\n",
        "        return []\n",
        "\n",
        "async def compute_accuracy_query_type4(queries, prompt_level, local_search):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    accuracy_results = {'accuracy': [], 'precision': [], 'recall': [], 'f1_score': []}\n",
        "    response_results = {'results': []}\n",
        "\n",
        "\n",
        "    for query_data in queries:\n",
        "        query = query_data['query']\n",
        "        predicate = query_data['predicate']\n",
        "        object1 = query_data['object']\n",
        "\n",
        "        ground_truth_triples = await query_sparql_type4(predicate, object1)\n",
        "        print(f\"\\nQuery: {query}:\")\n",
        "        response = await local_search.asearch(query=query, search_prompt=prompt_level)\n",
        "        response_text = response.response\n",
        "\n",
        "        model_response_triples = extract_triples_formatted(response_text)\n",
        "        print(model_response_triples)\n",
        "\n",
        "        common, extra, precision, recall, f1_score = find_common_and_extra_elements(ground_truth_triples, model_response_triples)\n",
        "\n",
        "        accuracy_results['precision'].append(precision)\n",
        "        accuracy_results['recall'].append(recall)\n",
        "        accuracy_results['f1_score'].append(f1_score)\n",
        "        response_results['results'].append(response_text)\n",
        "\n",
        "\n",
        "        print(\"Common Triples:\", common)\n",
        "        print(\"Extra Triples:\", extra)\n",
        "        print(f\"Precision: {precision:.2f}, Recall: {recall:.2f}, F1-score: {f1_score:.2f}\")\n",
        "        print(f\"Response: {response_text}\\n\")\n",
        "\n",
        "    mean_metrics = {\n",
        "        'precision': np.mean(accuracy_results['precision']) if accuracy_results['precision'] else 0,\n",
        "        'recall': np.mean(accuracy_results['recall']) if accuracy_results['recall'] else 0,\n",
        "        'f1_score': np.mean(accuracy_results['f1_score']) if accuracy_results['f1_score'] else 0,\n",
        "    }\n",
        "\n",
        "    return accuracy_results, mean_metrics, response_results\n",
        "\n",
        "queries_type4 = [\n",
        "    {\n",
        "        \"query\": \"Find the subject where the predicate is 'affects' and the object is 'cell_function'.\",\n",
        "        \"predicate\": \"affects\",\n",
        "        \"object\": \"cell_function\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"Find the subject where the predicate is 'complicates' and the object is 'cell_function'.\",\n",
        "        \"predicate\": \"complicates\",\n",
        "        \"object\": \"cell_function\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"Find the subject where the predicate is 'disrupts' and the object is 'cell_function'.\",\n",
        "        \"predicate\": \"disrupts\",\n",
        "        \"object\": \"cell_function\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"Find the subject where the predicate is 'affects' and the object is 'genetic_function'.\",\n",
        "        \"predicate\": \"affects\",\n",
        "        \"object\": \"genetic_function\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"Find the subject where the predicate is 'manifestation_of' and the object is 'genetic_function'.\",\n",
        "        \"predicate\": \"manifestation_of\",\n",
        "        \"object\": \"genetic_function\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"Find the subject where the predicate is 'result_of' and the object is 'genetic_function'.\",\n",
        "        \"predicate\": \"result_of\",\n",
        "        \"object\": \"genetic_function\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"Find the subject where the predicate is 'complicates' and the object is 'cell_or_molecular_dysfunction'.\",\n",
        "        \"predicate\": \"complicates\",\n",
        "        \"object\": \"cell_or_molecular_dysfunction\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"Find the subject where the predicate is 'treats' and the object is 'cell_or_molecular_dysfunction'.\",\n",
        "        \"predicate\": \"treats\",\n",
        "        \"object\": \"cell_or_molecular_dysfunction\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"Find the subject where the predicate is 'affects' and the object is 'organ_or_tissue_function'.\",\n",
        "        \"predicate\": \"affects\",\n",
        "        \"object\": \"organ_or_tissue_function\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"Find the subject where the predicate is 'precedes' and the object is 'organ_or_tissue_function'.\",\n",
        "        \"predicate\": \"precedes\",\n",
        "        \"object\": \"organ_or_tissue_function\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"Find the subject where the predicate is 'result_of' and the object is 'organ_or_tissue_function'.\",\n",
        "        \"predicate\": \"result_of\",\n",
        "        \"object\": \"organ_or_tissue_function\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"Find the subject where the predicate is 'process_of' and the object is 'organ_or_tissue_function'.\",\n",
        "        \"predicate\": \"process_of\",\n",
        "        \"object\": \"organ_or_tissue_function\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"Find the subject where the predicate is 'part_of' and the object is 'organism'.\",\n",
        "        \"predicate\": \"part_of\",\n",
        "        \"object\": \"organism\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"Find the subject where the predicate is 'affects' and the object is 'mental_or_behavioral_dysfunction'.\",\n",
        "        \"predicate\": \"affects\",\n",
        "        \"object\": \"mental_or_behavioral_dysfunction\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"Find the subject where the predicate is 'associated_with' and the object is 'mental_or_behavioral_dysfunction'.\",\n",
        "        \"predicate\": \"associated_with\",\n",
        "        \"object\": \"mental_or_behavioral_dysfunction\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"Find the subject where the predicate is 'precedes' and the object is 'mental_or_behavioral_dysfunction'.\",\n",
        "        \"predicate\": \"precedes\",\n",
        "        \"object\": \"mental_or_behavioral_dysfunction\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"Find the subject where the predicate is 'complicates' and the object is 'mental_or_behavioral_dysfunction'.\",\n",
        "        \"predicate\": \"complicates\",\n",
        "        \"object\": \"mental_or_behavioral_dysfunction\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"Find the subject where the predicate is 'manifestation_of' and the object is 'mental_or_behavioral_dysfunction'.\",\n",
        "        \"predicate\": \"manifestation_of\",\n",
        "        \"object\": \"mental_or_behavioral_dysfunction\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"Find the subject where the predicate is 'process_of' and the object is 'mental_or_behavioral_dysfunction'.\",\n",
        "        \"predicate\": \"process_of\",\n",
        "        \"object\": \"mental_or_behavioral_dysfunction\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"Find the subject where the predicate is 'affects' and the object is 'disease_or_syndrome'.\",\n",
        "        \"predicate\": \"affects\",\n",
        "        \"object\": \"disease_or_syndrome\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"Find the subject where the predicate is 'diagnoses' and the object is 'disease_or_syndrome'.\",\n",
        "        \"predicate\": \"diagnoses\",\n",
        "        \"object\": \"disease_or_syndrome\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"Find the subject where the predicate is 'measures' and the object is 'disease_or_syndrome'.\",\n",
        "        \"predicate\": \"measures\",\n",
        "        \"object\": \"disease_or_syndrome\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"Find the subject where the predicate is 'process_of' and the object is 'cell_function'.\",\n",
        "        \"predicate\": \"process_of\",\n",
        "        \"object\": \"cell_function\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"Find the subject where the predicate is 'result_of' and the object is 'cell_function'.\",\n",
        "        \"predicate\": \"result_of\",\n",
        "        \"object\": \"cell_function\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"Find the subject where the predicate is 'precedes' and the object is 'cell_function'.\",\n",
        "        \"predicate\": \"precedes\",\n",
        "        \"object\": \"cell_function\"\n",
        "    }\n",
        "]\n"
      ],
      "metadata": {
        "id": "ODiwLNnh8Xgx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "async def query_sparql_type3(subject, obj):\n",
        "    \"\"\"Queries the SPARQL endpoint with a dynamic subject and predicate and extracts triples.\"\"\"\n",
        "    sparql = SPARQLWrapper(SPARQL_ENDPOINT)\n",
        "    sparql.setReturnFormat(JSON)\n",
        "\n",
        "    query = f\"\"\"\n",
        "    PREFIX rag: <http://graphrag.com/>\n",
        "\n",
        "    SELECT ?subject ?predicate ?object\n",
        "    WHERE {{ rag:{subject} ?predicate rag:{obj}.\n",
        "        BIND(rag:{subject} AS ?subject)\n",
        "        BIND(rag:{obj} AS ?object)\n",
        "      }}\n",
        "    \"\"\"\n",
        "    sparql.setQuery(query)\n",
        "\n",
        "    try:\n",
        "        ret = sparql.queryAndConvert()\n",
        "        return extract_triples(ret)\n",
        "    except Exception as e:\n",
        "        print(f\"Error querying SPARQL: {e}\")\n",
        "        return []\n",
        "\n",
        "async def compute_accuracy_for_query3(queries, prompt_level, local_search):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    accuracy_results = {'accuracy': [], 'precision': [], 'recall': [], 'f1_score': []}\n",
        "    response_results = {'results': []}\n",
        "\n",
        "    for query_data in queries:\n",
        "        query = query_data['query']\n",
        "        object2 = query_data['subject']\n",
        "        object1 = query_data['object']\n",
        "\n",
        "        ground_truth_triples = await query_sparql_type3(object2, object1)\n",
        "\n",
        "        print(f\"\\nQuery: {query}:\")\n",
        "        response = await local_search.asearch(query=query, search_prompt=prompt_level)\n",
        "        response_text = response.response\n",
        "\n",
        "\n",
        "        model_response_triples = extract_triples_formatted(response_text)\n",
        "        print(model_response_triples)\n",
        "\n",
        "        common, extra, precision, recall, f1_score = find_common_and_extra_elements(ground_truth_triples, model_response_triples)\n",
        "\n",
        "        accuracy_results['precision'].append(precision)\n",
        "        accuracy_results['recall'].append(recall)\n",
        "        accuracy_results['f1_score'].append(f1_score)\n",
        "        response_results['results'].append(response_text)\n",
        "\n",
        "\n",
        "        print(\"Common Triples:\", common)\n",
        "        print(\"Extra Triples:\", extra)\n",
        "        print(f\"Precision: {precision:.2f}, Recall: {recall:.2f}, F1-score: {f1_score:.2f}\")\n",
        "        print(f\"Response: {response_text}\\n\")\n",
        "\n",
        "    mean_metrics = {\n",
        "        'precision': np.mean(accuracy_results['precision']) if accuracy_results['precision'] else 0,\n",
        "        'recall': np.mean(accuracy_results['recall']) if accuracy_results['recall'] else 0,\n",
        "        'f1_score': np.mean(accuracy_results['f1_score']) if accuracy_results['f1_score'] else 0,\n",
        "    }\n",
        "\n",
        "    return accuracy_results, mean_metrics, response_results\n",
        "\n",
        "\n",
        "queries_type3 = [\n",
        "    {\n",
        "        \"query\": \"Retrieve the relationships that connect vitamin and cell_function\",\n",
        "        \"subject\": \"vitamin\",\n",
        "        \"object\": \"cell_function\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"Retrieve the relationships that connect disease_or_syndrome and genetic_function\",\n",
        "        \"subject\": \"disease_or_syndrome\",\n",
        "        \"object\": \"genetic_function\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"Retrieve the relationships that connect antibiotic and cell_or_molecular_dysfunction\",\n",
        "        \"subject\": \"antibiotic\",\n",
        "        \"object\": \"cell_or_molecular_dysfunction\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"Retrieve the relationships that connect cell_function and organ_or_tissue_function\",\n",
        "        \"subject\": \"cell_function\",\n",
        "        \"object\": \"organ_or_tissue_function\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"Retrieve the relationships that connect body_part_organ_or_organ_component and organism\",\n",
        "        \"subject\": \"body_part_organ_or_organ_component\",\n",
        "        \"object\": \"organism\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"Retrieve the relationships that connect disease_or_syndrome and mental_or_behavioral_dysfunction\",\n",
        "        \"subject\": \"disease_or_syndrome\",\n",
        "        \"object\": \"mental_or_behavioral_dysfunction\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"Retrieve the relationships that connect chemical and disease_or_syndrome\",\n",
        "        \"subject\": \"chemical\",\n",
        "        \"object\": \"disease_or_syndrome\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"Retrieve the relationships that connect physiological_function and cell_function\",\n",
        "        \"subject\": \"physiologic_function\",\n",
        "        \"object\": \"cell_function\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"Retrieve the relationships that connect laboratory_procedure and disease_or_syndrome\",\n",
        "        \"subject\": \"laboratory_procedure\",\n",
        "        \"object\": \"disease_or_syndrome\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"Retrieve the relationships that connect mental_process and cell_function\",\n",
        "        \"subject\": \"mental_process\",\n",
        "        \"object\": \"cell_function\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"Retrieve the relationships that connect tissue and antibiotic\",\n",
        "        \"subject\": \"tissue\",\n",
        "        \"object\": \"antibiotic\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"Retrieve the relationships that connect invertebrate and behavior\",\n",
        "        \"subject\": \"invertebrate\",\n",
        "        \"object\": \"behavior\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"Retrieve the relationships that connect age_group and classification\",\n",
        "        \"subject\": \"age_group\",\n",
        "        \"object\": \"classification\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"Retrieve the relationships that connect biologic_function and hormone\",\n",
        "        \"subject\": \"biologic_function\",\n",
        "        \"object\": \"hormone\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"Retrieve the relationships that connect carbohydrate and lipid\",\n",
        "        \"subject\": \"carbohydrate\",\n",
        "        \"object\": \"lipid\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"Retrieve the relationships that connect diagnostic_procedure and nucleic_acid_nucleoside_or_nucleotide\",\n",
        "        \"subject\": \"diagnostic_procedure\",\n",
        "        \"object\": \"nucleic_acid_nucleoside_or_nucleotide\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"Retrieve the relationships that connect body_substance and organophosphorus_compound\",\n",
        "        \"subject\": \"body_substance\",\n",
        "        \"object\": \"organophosphorus_compound\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"Retrieve the relationships that connect age_group and research_device\",\n",
        "        \"subject\": \"age_group\",\n",
        "        \"object\": \"research_device\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"Retrieve the relationships that connect molecular_biology_research_technique and event\",\n",
        "        \"subject\": \"molecular_biology_research_technique\",\n",
        "        \"object\": \"event\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"Retrieve the relationships that connect laboratory_procedure and inorganic_chemical\",\n",
        "        \"subject\": \"laboratory_procedure\",\n",
        "        \"object\": \"inorganic_chemical\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"Retrieve the relationships that connect fully_formed_anatomical_structure and steroid\",\n",
        "        \"subject\": \"fully_formed_anatomical_structure\",\n",
        "        \"object\": \"steroid\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"Retrieve the relationships that connect fish and occupation_or_discipline\",\n",
        "        \"subject\": \"fish\",\n",
        "        \"object\": \"occupation_or_discipline\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"Retrieve the relationships that connect embryonic_structure and rickettsia_or_chlamydia\",\n",
        "        \"subject\": \"embryonic_structure\",\n",
        "        \"object\": \"rickettsia_or_chlamydia\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"Retrieve the relationships that connect self_help_or_relief_organization and governmental_or_regulatory_activity\",\n",
        "        \"subject\": \"self_help_or_relief_organization\",\n",
        "        \"object\": \"governmental_or_regulatory_activity\"\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "AWu7r5RQ8p_9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "from SPARQLWrapper import SPARQLWrapper, JSON\n",
        "import numpy as np\n",
        "\n",
        "from SPARQLWrapper import SPARQLWrapper, JSON\n",
        "\n",
        "async def query_sparql_type2(object2, predicate):\n",
        "    \"\"\"Queries the SPARQL endpoint with a dynamic subject and predicate and extracts triples.\"\"\"\n",
        "    sparql = SPARQLWrapper(SPARQL_ENDPOINT)\n",
        "    sparql.setReturnFormat(JSON)\n",
        "\n",
        "    query = f\"\"\"\n",
        "    PREFIX rag: <http://graphrag.com/>\n",
        "    SELECT ?subject ?predicate ?object\n",
        "    WHERE {{\n",
        "        {{ rag:{object2} rag:{predicate} ?object .\n",
        "           BIND(rag:{object2} AS ?subject)\n",
        "           BIND(rag:{predicate} AS ?predicate)\n",
        "        }}\n",
        "        UNION\n",
        "        {{ ?subject rag:{predicate} rag:{object2} .\n",
        "           BIND(rag:{object2} AS ?object)\n",
        "           BIND(rag:{predicate} AS ?predicate)\n",
        "        }}\n",
        "    }}\n",
        "    \"\"\"\n",
        "\n",
        "    sparql.setQuery(query)\n",
        "\n",
        "    try:\n",
        "        ret = sparql.queryAndConvert()\n",
        "        return extract_triples(ret)\n",
        "    except Exception as e:\n",
        "        print(f\"Error querying SPARQL: {e}\")\n",
        "        return []\n",
        "\n",
        "\n",
        "async def compute_accuracy_for_query2(queries, prompt_level, local_search):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    accuracy_results = {'accuracy': [], 'precision': [], 'recall': [], 'f1_score': []}\n",
        "    response_results = {'results': []}\n",
        "\n",
        "    for query_data in queries:\n",
        "        query = query_data['query']\n",
        "        object2 = query_data['object']\n",
        "        predicate = query_data['predicate']\n",
        "\n",
        "        ground_truth_triples = await query_sparql_type2(object2, predicate)\n",
        "\n",
        "        print(f\"\\nQuery: {query}:\")\n",
        "        response = await local_search.asearch(query=query, search_prompt=prompt_level)\n",
        "        response_text = response.response\n",
        "        model_response_triples = extract_triples_formatted(response_text)\n",
        "        print(model_response_triples)\n",
        "\n",
        "        common, extra, precision, recall, f1_score = find_common_and_extra_elements(ground_truth_triples, model_response_triples)\n",
        "\n",
        "        accuracy_results['precision'].append(precision)\n",
        "        accuracy_results['recall'].append(recall)\n",
        "        accuracy_results['f1_score'].append(f1_score)\n",
        "        response_results['results'].append(response_text)\n",
        "\n",
        "\n",
        "        print(\"Common Triples:\", common)\n",
        "        print(\"Extra Triples:\", extra)\n",
        "        print(f\"Precision: {precision:.2f}, Recall: {recall:.2f}, F1-score: {f1_score:.2f}\")\n",
        "        print(f\"Response: {response_text}\\n\")\n",
        "\n",
        "\n",
        "    mean_metrics = {\n",
        "        'precision': np.mean(accuracy_results['precision']) if accuracy_results['precision'] else 0,\n",
        "        'recall': np.mean(accuracy_results['recall']) if accuracy_results['recall'] else 0,\n",
        "        'f1_score': np.mean(accuracy_results['f1_score']) if accuracy_results['f1_score'] else 0,\n",
        "    }\n",
        "\n",
        "    return accuracy_results, mean_metrics, response_results\n",
        "\n",
        "\n",
        "\n",
        "queries_type_2 = [\n",
        "    {\n",
        "        \"query\": \"Retrieve all triples where the predicate is 'result_of', and the subject OR the object is 'anatomical_abnormality'.\",\n",
        "        \"predicate\": \"result_of\",\n",
        "        \"object\": \"anatomical_abnormality\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"Retrieve all triples where the predicate is 'affects', and the subject OR the object is 'cell_function'.\",\n",
        "        \"predicate\": \"affects\",\n",
        "        \"object\": \"cell_function\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"Retrieve all triples where the predicate is 'affects', and the subject OR the object is 'disease_or_syndrome'.\",\n",
        "        \"predicate\": \"affects\",\n",
        "        \"object\": \"disease_or_syndrome\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"Retrieve all triples where the predicate is 'manifestation_of', and the subject OR the object is 'disease_or_syndrome'.\",\n",
        "        \"predicate\": \"manifestation_of\",\n",
        "        \"object\": \"disease_or_syndrome\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"Retrieve all triples where the predicate is 'process_of', and the subject OR the object is 'genetic_function'.\",\n",
        "        \"predicate\": \"process_of\",\n",
        "        \"object\": \"genetic_function\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"Retrieve all triples where the predicate is 'interacts_with', and the subject OR the object is 'bacterium'.\",\n",
        "        \"predicate\": \"interacts_with\",\n",
        "        \"object\": \"bacterium\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"Retrieve all triples where the predicate is 'interacts_with', and the subject OR the object is 'bird'.\",\n",
        "        \"predicate\": \"interacts_with\",\n",
        "        \"object\": \"bird\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"Retrieve all triples where the predicate is 'part_of', and the subject OR the object is 'body_part_organ_or_organ_component'.\",\n",
        "        \"predicate\": \"part_of\",\n",
        "        \"object\": \"body_part_organ_or_organ_component\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"Retrieve all triples where the predicate is 'issue_in', and the subject OR the object is 'biomedical_occupation_or_discipline'.\",\n",
        "        \"predicate\": \"issue_in\",\n",
        "        \"object\": \"biomedical_occupation_or_discipline\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"Retrieve all triples where the predicate is 'adjacent_to', and the subject OR the object is 'body_part_organ_or_organ_component'.\",\n",
        "        \"predicate\": \"adjacent_to\",\n",
        "        \"object\": \"body_part_organ_or_organ_component\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"Retrieve all triples where the predicate is 'prevents', and the subject OR the object is 'antibiotic'.\",\n",
        "        \"predicate\": \"prevents\",\n",
        "        \"object\": \"antibiotic\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"Retrieve all triples where the predicate is 'complicates', and the subject OR the object is 'disease_or_syndrome'.\",\n",
        "        \"predicate\": \"complicates\",\n",
        "        \"object\": \"disease_or_syndrome\"\n",
        "    },\n",
        "      {\n",
        "        \"query\": \"Retrieve all triples where the predicate is 'complicates', and the subject OR the object is 'neoplastic_process'.\",\n",
        "        \"predicate\": \"complicates\",\n",
        "        \"object\": \"neoplastic_process\"\n",
        "    },\n",
        "     {\n",
        "        \"query\": \"Retrieve all triples where the predicate is 'produces', and the subject OR the object is 'age_group'.\",\n",
        "        \"predicate\": \"produces\",\n",
        "        \"object\": \"age_group\"\n",
        "    },\n",
        "     {\n",
        "        \"query\": \"Retrieve all triples where the predicate is 'isa', and the subject OR the object is 'animal'.\",\n",
        "        \"predicate\": \"isa\",\n",
        "        \"object\": \"animal\"\n",
        "    },\n",
        "       {\n",
        "        \"query\": \"Retrieve all triples where the predicate is 'isa', and the subject OR the object is 'occupational_activity'.\",\n",
        "        \"predicate\": \"isa\",\n",
        "        \"object\": \"occupational_activity\"\n",
        "    },\n",
        "     {\n",
        "        \"query\": \"Retrieve all triples where the predicate is 'developmental_form_of', and the subject OR the object is 'tissue'.\",\n",
        "        \"predicate\": \"developmental_form_of\",\n",
        "        \"object\": \"tissue\"\n",
        "    },\n",
        "        {\n",
        "        \"query\": \"Retrieve all triples where the predicate is 'associated_with', and the subject OR the object is 'cell_or_molecular_dysfunction'.\",\n",
        "        \"predicate\": \"associated_with\",\n",
        "        \"object\": \"cell_or_molecular_dysfunction\"\n",
        "    },\n",
        "\n",
        "         {\n",
        "        \"query\": \"Retrieve all triples where the predicate is 'associated_with', and the subject OR the object is 'mental_or_behavioral_dysfunction'.\",\n",
        "        \"predicate\": \"associated_with\",\n",
        "        \"object\": \"mental_or_behavioral_dysfunction\"\n",
        "    },\n",
        "    \t {\n",
        "        \"query\": \"Retrieve all triples where the predicate is 'location_of', and the subject OR the object is 'fungus'.\",\n",
        "        \"predicate\": \"location_of\",\n",
        "        \"object\": \"fungus\"\n",
        "    },\n",
        "     {\n",
        "        \"query\": \"Retrieve all triples where the predicate is 'location_of', and the subject OR the object is 'virus'.\",\n",
        "        \"predicate\": \"location_of\",\n",
        "        \"object\": \"virus\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"Retrieve all triples where the predicate is 'surrounds', and the subject OR the object is 'body_substance'.\",\n",
        "        \"predicate\": \"surrounds\",\n",
        "        \"object\": \"body_substance\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"Retrieve all triples where the predicate is 'surrounds', and the subject OR the object is 'tissue'.\",\n",
        "        \"predicate\": \"surrounds\",\n",
        "        \"object\": \"tissue\"\n",
        "    },\n",
        "       {\n",
        "        \"query\": \"Retrieve all triples where the predicate is 'produces', and the subject OR the object is 'cell_component'.\",\n",
        "        \"predicate\": \"produces\",\n",
        "        \"object\": \"cell_component\"\n",
        "    },\n",
        "       {\n",
        "        \"query\": \"Retrieve all triples where the predicate is 'manages', and the subject OR the object is 'self_help_or_relief_organization'.\",\n",
        "        \"predicate\": \"manages\",\n",
        "        \"object\": \"self_help_or_relief_organization\"\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "VnW-C44t9Mvs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "from SPARQLWrapper import SPARQLWrapper, JSON\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "async def query_sparql_type1(subject, predicate):\n",
        "    \"\"\"Queries the SPARQL endpoint with a dynamic subject and predicate and extracts triples.\"\"\"\n",
        "    sparql = SPARQLWrapper(SPARQL_ENDPOINT)\n",
        "    sparql.setReturnFormat(JSON)\n",
        "\n",
        "    query = f\"\"\"\n",
        "    PREFIX rag: <http://graphrag.com/>\n",
        "    SELECT ?subject ?predicate ?object\n",
        "    WHERE {{\n",
        "      rag:{subject} rag:{predicate} ?object.\n",
        "      BIND(rag:{predicate} AS ?predicate)\n",
        "      BIND(rag:{subject} AS ?subject)\n",
        "    }}\n",
        "    \"\"\"\n",
        "    sparql.setQuery(query)\n",
        "\n",
        "    try:\n",
        "        ret = sparql.queryAndConvert()\n",
        "        return extract_triples(ret)\n",
        "    except Exception as e:\n",
        "        print(f\"Error querying SPARQL: {e}\")\n",
        "        return []\n",
        "\n",
        "async def compute_accuracy_for_query_type1(queries, prompt_level, local_search):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    accuracy_results = {'accuracy': [], 'precision': [], 'recall': [], 'f1_score': []}\n",
        "    response_results = {'results': []}\n",
        "\n",
        "    for query_data in queries:\n",
        "        query = query_data['query']\n",
        "        subject = query_data['subject']\n",
        "        predicate = query_data['predicate']\n",
        "\n",
        "        ground_truth_triples = await query_sparql_type1(subject, predicate)\n",
        "\n",
        "        print(f\"\\nQuery: {query}:\")\n",
        "        response = await local_search.asearch(query=query, search_prompt=prompt_level)\n",
        "        response_text = response.response\n",
        "\n",
        "        model_response_triples = extract_triples_formatted(response_text)\n",
        "        print(model_response_triples)\n",
        "        common, extra, precision, recall, f1_score = find_common_and_extra_elements(ground_truth_triples, model_response_triples)\n",
        "\n",
        "        accuracy_results['precision'].append(precision)\n",
        "        accuracy_results['recall'].append(recall)\n",
        "        accuracy_results['f1_score'].append(f1_score)\n",
        "        response_results['results'].append(response_text)\n",
        "\n",
        "\n",
        "        print(\"Common Triples:\", common)\n",
        "        print(\"Extra Triples:\", extra)\n",
        "        print(f\"Precision: {precision:.2f}, Recall: {recall:.2f}, F1-score: {f1_score:.2f}\")\n",
        "        print(f\"Response: {response_text}\\n\")\n",
        "\n",
        "    mean_metrics = {\n",
        "        'precision': np.mean(accuracy_results['precision']) if accuracy_results['precision'] else 0,\n",
        "        'recall': np.mean(accuracy_results['recall']) if accuracy_results['recall'] else 0,\n",
        "        'f1_score': np.mean(accuracy_results['f1_score']) if accuracy_results['f1_score'] else 0,\n",
        "    }\n",
        "\n",
        "    return accuracy_results, mean_metrics, response_results\n",
        "\n",
        "queries_type_1 = [\n",
        "    {\"query\": \"Retrieve all triples where the predicate is 'affects', and the subject is 'vitamin'.\", \"subject\": \"vitamin\", \"predicate\": \"affects\"},\n",
        "    {\"query\": \"Retrieve all triples where the predicate is 'affects', and the subject is 'behavior'.\", \"subject\": \"behavior\", \"predicate\": \"affects\"},\n",
        "    {\"query\": \"Retrieve all triples where the predicate is 'result_of', and the subject is 'disease_or_syndrome'.\", \"subject\": \"disease_or_syndrome\", \"predicate\": \"result_of\"},\n",
        "    {\"query\": \"Retrieve all triples where the predicate is 'ingredient_of', and the subject is 'carbohydrate'.\", \"subject\": \"carbohydrate\", \"predicate\": \"ingredient_of\"},\n",
        "    {\"query\": \"Retrieve all triples where the predicate is 'issue_in', and the subject is 'steroid'.\", \"subject\": \"steroid\", \"predicate\": \"issue_in\"},\n",
        "    {\"query\": \"Retrieve all triples where the predicate is 'exhibits', and the subject is 'vertebrate'.\", \"subject\": \"vertebrate\", \"predicate\": \"exhibits\"},\n",
        "    {\"query\": \"Retrieve all triples where the predicate is 'associated_with', and the subject is 'social_behavior'.\", \"subject\": \"social_behavior\", \"predicate\": \"associated_with\"},\n",
        "    {\"query\": \"Retrieve all triples where the predicate is 'complicates', and the subject is 'antibiotic'.\", \"subject\": \"antibiotic\", \"predicate\": \"complicates\"},\n",
        "    {\"query\": \"Retrieve all triples where the predicate is 'consists_of', and the subject is 'body_substance'.\", \"subject\": \"body_substance\", \"predicate\": \"consists_of\"},\n",
        "    {\"query\": \"Retrieve all triples where the predicate is 'derivative_of', and the subject is 'body_substance'.\", \"subject\": \"body_substance\", \"predicate\": \"derivative_of\"},\n",
        "    {\"query\": \"Retrieve all triples where the predicate is 'precedes', and the subject is 'cell_function'.\", \"subject\": \"cell_function\", \"predicate\": \"precedes\"},\n",
        "    {\"query\": \"Retrieve all triples where the predicate is 'issue_in', and the subject is 'chemical_viewed_structurally'.\", \"subject\": \"chemical_viewed_structurally\", \"predicate\": \"issue_in\"},\n",
        "    {\"query\": \"Retrieve all triples where the predicate is 'exhibits', and the subject is 'vertebrate'.\", \"subject\": \"vertebrate\", \"predicate\": \"exhibits\"},\n",
        "    {\"query\": \"Retrieve all triples where the predicate is 'uses', and the subject is 'age_group'.\", \"subject\": \"age_group\", \"predicate\": \"uses\"},\n",
        "    {\"query\": \"Retrieve all triples where the predicate is 'property_of', and the subject is 'amino_acid_sequence'.\", \"subject\": \"amino_acid_sequence\", \"predicate\": \"property_of\"},\n",
        "    {\"query\": \"Retrieve all triples where the predicate is 'treats', and the subject is 'antibiotic'.\", \"subject\": \"antibiotic\", \"predicate\": \"treats\"},\n",
        "    {\"query\": \"Retrieve all triples where the predicate is 'interconnects', and the subject is 'body_part_organ_or_organ_component'.\", \"subject\": \"body_part_organ_or_organ_component\", \"predicate\": \"interconnects\"},\n",
        "    {\"query\": \"Retrieve all triples where the predicate is 'derivative_of', and the subject is 'body_substance'.\", \"subject\": \"body_substance\", \"predicate\": \"derivative_of\"},\n",
        "    {\"query\": \"Retrieve all triples where the predicate is 'adjacent_to', and the subject is 'cell_component'.\", \"subject\": \"cell_component\", \"predicate\": \"adjacent_to\"},\n",
        "    {\"query\": \"Retrieve all triples where the predicate is 'uses', and the subject is 'diagnostic_procedure'.\", \"subject\": \"diagnostic_procedure\", \"predicate\": \"uses\"},\n",
        "    {\"query\": \"Retrieve all triples where the predicate is 'developmental_form_of', and the subject is 'embryonic_structure'.\", \"subject\": \"embryonic_structure\", \"predicate\": \"developmental_form_of\"},\n",
        "    {\"query\": \"Retrieve all triples where the predicate is 'indicates', and the subject is 'laboratory_or_test_result'.\", \"subject\": \"laboratory_or_test_result\", \"predicate\": \"indicates\"},\n",
        "    {\"query\": \"Retrieve all triples where the predicate is 'surrounds', and the subject is 'tissue'.\", \"subject\": \"tissue\", \"predicate\": \"surrounds\"},\n",
        "    {\"query\": \"Retrieve all triples where the predicate is 'prevents', and the subject is 'therapeutic_or_preventive_procedure'.\", \"subject\": \"therapeutic_or_preventive_procedure\", \"predicate\": \"prevents\"},\n",
        "    {\"query\": \"Retrieve all triples where the predicate is 'treats', and the subject is 'therapeutic_or_preventive_procedure'.\", \"subject\": \"therapeutic_or_preventive_procedure\", \"predicate\": \"treats\"}\n",
        "]"
      ],
      "metadata": {
        "id": "DrBumHYm9dxt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import httpx\n",
        "from transformers import AutoTokenizer\n",
        "from google.colab import userdata\n",
        "import requests\n",
        "\n",
        "async def get_model_response(query, prompt_level, context_data):\n",
        "    model = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model)  # Keep this synchronous\n",
        "\n",
        "    search_prompt = (\n",
        "        f\"{prompt_level}\\n\"\n",
        "        f\"Query: {query}\\n\\n\"\n",
        "        f\"Context:\\n{context_data}\\n\\n\"\n",
        "    )\n",
        "\n",
        "    messages = [\n",
        "        {'role': 'system', 'content': 'You are an expert in generating RDF triples'},\n",
        "        {'role': 'user', 'content': search_prompt}\n",
        "    ]\n",
        "\n",
        "    tokenized_chat = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "    API_URL = f\"https://api-inference.huggingface.co/models/{model}\"\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {userdata.get('HF_TOKEN')}\",\n",
        "        \"x-compute-type\": \"cpu+optimized\"\n",
        "    }\n",
        "\n",
        "    payload = {\n",
        "        'inputs': tokenized_chat,\n",
        "        'parameters': {'return_full_text': False, 'max_new_tokens': 256},  # Limit token output\n",
        "        'options': {'use_cache': False}\n",
        "    }\n",
        "\n",
        "    async with httpx.AsyncClient() as client:\n",
        "        response = await client.post(API_URL, headers=headers, json=payload, timeout=180)\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            return response.json()[0]['generated_text']\n",
        "        else:\n",
        "            return f\"Error: {response.status_code}, {response.text}\""
      ],
      "metadata": {
        "id": "mvn91JQi_W14"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "async def compute_accuracy_for_query_type1_mixtral(queries, prompt_level):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    accuracy_results = {'accuracy': [], 'precision': [], 'recall': [], 'f1_score': []}\n",
        "    response_results = {'results': []}\n",
        "\n",
        "    for query_data in queries:\n",
        "        query = query_data['query']\n",
        "        subject = query_data['subject']\n",
        "        predicate = query_data['predicate']\n",
        "\n",
        "        ground_truth_triples = await query_sparql_type1(subject, predicate)\n",
        "\n",
        "        print(f\"\\nQuery: {query}:\")\n",
        "        context = context_builder.build_context(query=query).context_chunks\n",
        "        response_text = asyncio.run(get_model_response(query, prompt_level, context))\n",
        "        model_response_triples = extract_triples_formatted(response_text)\n",
        "        print(model_response_triples)\n",
        "\n",
        "        common, extra, precision, recall, f1_score = find_common_and_extra_elements(ground_truth_triples, model_response_triples)\n",
        "        accuracy_results['precision'].append(precision)\n",
        "        accuracy_results['recall'].append(recall)\n",
        "        accuracy_results['f1_score'].append(f1_score)\n",
        "        response_results['results'].append(response_text)\n",
        "\n",
        "        print(\"Common Triples:\", common)\n",
        "        print(\"Extra Triples:\", extra)\n",
        "        print(f\"Precision: {precision:.2f}, Recall: {recall:.2f}, F1-score: {f1_score:.2f}\")\n",
        "        print(f\"Response: {response_text}\\n\")\n",
        "\n",
        "\n",
        "    mean_metrics = {\n",
        "        'precision': np.mean(accuracy_results['precision']) if accuracy_results['precision'] else 0,\n",
        "        'recall': np.mean(accuracy_results['recall']) if accuracy_results['recall'] else 0,\n",
        "        'f1_score': np.mean(accuracy_results['f1_score']) if accuracy_results['f1_score'] else 0,\n",
        "    }\n",
        "\n",
        "    return accuracy_results, mean_metrics, response_results\n"
      ],
      "metadata": {
        "id": "uK8jzyxK_po2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "async def compute_accuracy_for_query2_mixtral(queries, prompt_level):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    accuracy_results = {'accuracy': [], 'precision': [], 'recall': [], 'f1_score': []}\n",
        "    response_results = {'results': []}\n",
        "\n",
        "    for query_data in queries:\n",
        "        query = query_data['query']\n",
        "        object2 = query_data['object']\n",
        "        predicate = query_data['predicate']\n",
        "\n",
        "        ground_truth_triples = await query_sparql_type2(object2, predicate)\n",
        "\n",
        "        print(f\"\\nQuery: {query}:\")\n",
        "\n",
        "        context = context_builder.build_context(query=query).context_chunks\n",
        "        response_text =  asyncio.run(get_model_response(query, prompt_level, context))\n",
        "\n",
        "\n",
        "        model_response_triples = extract_triples_formatted(response_text)\n",
        "        print(model_response_triples)\n",
        "\n",
        "        common, extra, precision, recall, f1_score = find_common_and_extra_elements(ground_truth_triples, model_response_triples)\n",
        "\n",
        "        accuracy_results['precision'].append(precision)\n",
        "        accuracy_results['recall'].append(recall)\n",
        "        accuracy_results['f1_score'].append(f1_score)\n",
        "        response_results['results'].append(response_text)\n",
        "\n",
        "\n",
        "        print(\"Common Triples:\", common)\n",
        "        print(\"Extra Triples:\", extra)\n",
        "        print(f\"Precision: {precision:.2f}, Recall: {recall:.2f}, F1-score: {f1_score:.2f}\")\n",
        "        print(f\"Response: {response_text}\\n\")\n",
        "\n",
        "    mean_metrics = {\n",
        "        'precision': np.mean(accuracy_results['precision']) if accuracy_results['precision'] else 0,\n",
        "        'recall': np.mean(accuracy_results['recall']) if accuracy_results['recall'] else 0,\n",
        "        'f1_score': np.mean(accuracy_results['f1_score']) if accuracy_results['f1_score'] else 0,\n",
        "    }\n",
        "\n",
        "    return accuracy_results, mean_metrics, response_results\n"
      ],
      "metadata": {
        "id": "ZmZc2dfFALDY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "async def compute_accuracy_for_query3_mixtral(queries, prompt_level):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    accuracy_results = {'accuracy': [], 'precision': [], 'recall': [], 'f1_score': []}\n",
        "    response_results = {'results': []}\n",
        "\n",
        "    for query_data in queries:\n",
        "        query = query_data['query']\n",
        "        object2 = query_data['subject']\n",
        "        object1 = query_data['object']\n",
        "\n",
        "        ground_truth_triples = await query_sparql_type3(object2, object1)\n",
        "        print(f\"\\nQuery: {query}:\")\n",
        "        context = context_builder.build_context(query=query).context_chunks\n",
        "        response_text =  asyncio.run(get_model_response(query, prompt_level, context))\n",
        "\n",
        "        model_response_triples = extract_triples_formatted(response_text)\n",
        "        print(model_response_triples)\n",
        "\n",
        "        common, extra, precision, recall, f1_score = find_common_and_extra_elements(ground_truth_triples, model_response_triples)\n",
        "\n",
        "        accuracy_results['precision'].append(precision)\n",
        "        accuracy_results['recall'].append(recall)\n",
        "        accuracy_results['f1_score'].append(f1_score)\n",
        "        response_results['results'].append(response_text)\n",
        "\n",
        "\n",
        "        print(\"Common Triples:\", common)\n",
        "        print(\"Extra Triples:\", extra)\n",
        "        print(f\"Precision: {precision:.2f}, Recall: {recall:.2f}, F1-score: {f1_score:.2f}\")\n",
        "        print(f\"Response: {response_text}\\n\")\n",
        "\n",
        "    mean_metrics = {\n",
        "        'precision': np.mean(accuracy_results['precision']) if accuracy_results['precision'] else 0,\n",
        "        'recall': np.mean(accuracy_results['recall']) if accuracy_results['recall'] else 0,\n",
        "        'f1_score': np.mean(accuracy_results['f1_score']) if accuracy_results['f1_score'] else 0,\n",
        "    }\n",
        "\n",
        "    return accuracy_results, mean_metrics, response_results\n"
      ],
      "metadata": {
        "id": "Fd6JoedsAhlP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "async def compute_accuracy_query_type4_mixtral(queries, prompt_level):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    accuracy_results = {'accuracy': [], 'precision': [], 'recall': [], 'f1_score': []}\n",
        "    response_results = {'results': []}\n",
        "\n",
        "\n",
        "    for query_data in queries:\n",
        "        query = query_data['query']\n",
        "        predicate = query_data['predicate']\n",
        "        object1 = query_data['object']\n",
        "\n",
        "        ground_truth_triples = await query_sparql_type4(predicate, object1)\n",
        "        print(f\"\\nQuery: {query}:\")\n",
        "        context = context_builder.build_context(query=query).context_chunks\n",
        "        response_text =  asyncio.run(get_model_response(query, prompt_level, context))\n",
        "        model_response_triples = extract_triples_formatted(response_text)\n",
        "        print(model_response_triples)\n",
        "        common, extra, precision, recall, f1_score = find_common_and_extra_elements(ground_truth_triples, model_response_triples)\n",
        "        accuracy_results['precision'].append(precision)\n",
        "        accuracy_results['recall'].append(recall)\n",
        "        accuracy_results['f1_score'].append(f1_score)\n",
        "        response_results['results'].append(response_text)\n",
        "        print(\"Common Triples:\", common)\n",
        "        print(\"Extra Triples:\", extra)\n",
        "        print(f\"Precision: {precision:.2f}, Recall: {recall:.2f}, F1-score: {f1_score:.2f}\")\n",
        "        print(f\"Response: {response_text}\\n\")\n",
        "    mean_metrics = {\n",
        "        'precision': np.mean(accuracy_results['precision']) if accuracy_results['precision'] else 0,\n",
        "        'recall': np.mean(accuracy_results['recall']) if accuracy_results['recall'] else 0,\n",
        "        'f1_score': np.mean(accuracy_results['f1_score']) if accuracy_results['f1_score'] else 0,\n",
        "    }\n",
        "\n",
        "    return accuracy_results, mean_metrics, response_results"
      ],
      "metadata": {
        "id": "iriCRDijAros"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "async def compute_accuracy_for_query_type5_mixtral(queries, prompt_level):\n",
        "    \"\"\"\n",
        "    Computes accuracy, precision, recall, and F1-score for query results.\n",
        "    \"\"\"\n",
        "    accuracy_results = {'accuracy': [], 'precision': [], 'recall': [], 'f1_score': []}\n",
        "    response_results = {'results': []}\n",
        "\n",
        "    for query_data in queries:\n",
        "        query = query_data['query']\n",
        "        subject = query_data['subject']\n",
        "\n",
        "        ground_truth_triples = await query_sparql_type5(subject)\n",
        "\n",
        "        print(f\"\\nQuery: {query}:\")\n",
        "        context = context_builder.build_context(query=query).context_chunks\n",
        "        response_text =  asyncio.run(get_model_response(query, prompt_level, context))\n",
        "\n",
        "        model_response_triples = extract_triples_formatted(response_text)\n",
        "        print(model_response_triples)\n",
        "\n",
        "        common, extra, precision, recall, f1_score = find_common_and_extra_elements(ground_truth_triples, model_response_triples)\n",
        "        accuracy_results['precision'].append(precision)\n",
        "        accuracy_results['recall'].append(recall)\n",
        "        accuracy_results['f1_score'].append(f1_score)\n",
        "        response_results['results'].append(response_text)\n",
        "\n",
        "\n",
        "        print(\"Common Triples:\", common)\n",
        "        print(\"Extra Triples:\", extra)\n",
        "        print(f\"Precision: {precision:.2f}, Recall: {recall:.2f}, F1-score: {f1_score:.2f}\")\n",
        "        print(f\"Response: {response_text}\\n\")\n",
        "\n",
        "    mean_metrics = {\n",
        "        'precision': np.mean(accuracy_results['precision']) if accuracy_results['precision'] else 0,\n",
        "        'recall': np.mean(accuracy_results['recall']) if accuracy_results['recall'] else 0,\n",
        "        'f1_score': np.mean(accuracy_results['f1_score']) if accuracy_results['f1_score'] else 0,\n",
        "    }\n",
        "\n",
        "    return accuracy_results, mean_metrics, response_results\n"
      ],
      "metadata": {
        "id": "GM4U_e-mA7Bh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "async def compute_accuracy_for_type6_mixtral(queries, prompt_level):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    accuracy_results = {'accuracy': [], 'precision': [], 'recall': [], 'f1_score': []}\n",
        "    response_results = {'results': []}\n",
        "\n",
        "    for query_data in queries:\n",
        "        query = query_data['query']\n",
        "        predicate = query_data['predicate']\n",
        "\n",
        "        ground_truth_triples = await query_sparql_type6(predicate)\n",
        "        print(f\"\\nQuery: {query}:\")\n",
        "        context = context_builder.build_context(query=query).context_chunks\n",
        "        response_text =  asyncio.run(get_model_response(query, prompt_level, context))\n",
        "\n",
        "        model_response_triples = extract_triples_formatted(response_text)\n",
        "        print(model_response_triples)\n",
        "\n",
        "        common, extra, precision, recall, f1_score = find_common_and_extra_elements(ground_truth_triples, model_response_triples)\n",
        "\n",
        "        accuracy_results['precision'].append(precision)\n",
        "        accuracy_results['recall'].append(recall)\n",
        "        accuracy_results['f1_score'].append(f1_score)\n",
        "        response_results['results'].append(response_text)\n",
        "\n",
        "        print(\"Common Triples:\", common)\n",
        "        print(\"Extra Triples:\", extra)\n",
        "        print(f\"Precision: {precision:.2f}, Recall: {recall:.2f}, F1-score: {f1_score:.2f}\")\n",
        "        print(f\"Response: {response_text}\\n\")\n",
        "\n",
        "\n",
        "    mean_metrics = {\n",
        "        'precision': np.mean(accuracy_results['precision']) if accuracy_results['precision'] else 0,\n",
        "        'recall': np.mean(accuracy_results['recall']) if accuracy_results['recall'] else 0,\n",
        "        'f1_score': np.mean(accuracy_results['f1_score']) if accuracy_results['f1_score'] else 0,\n",
        "    }\n",
        "\n",
        "    return accuracy_results, mean_metrics, response_results"
      ],
      "metadata": {
        "id": "fs8yfguWBJAw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "async def compute_accuracy_type7_mixtral(queries, prompt_level):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    accuracy_results = {'accuracy': [], 'precision': [], 'recall': [], 'f1_score': []}\n",
        "    response_results = {'results': []}\n",
        "\n",
        "    for query_data in queries:\n",
        "        query = query_data['query']\n",
        "        subject = query_data['subject']\n",
        "\n",
        "        ground_truth_triples = await query_sparql_type7(subject)\n",
        "        print(f\"\\nQuery: {query}:\")\n",
        "        context = context_builder.build_context(query=query).context_chunks\n",
        "        response_text =  asyncio.run(get_model_response(query, prompt_level, context))\n",
        "\n",
        "        model_response_triples = extract_triples_formatted(response_text)\n",
        "        print(model_response_triples)\n",
        "\n",
        "        common, extra, precision, recall, f1_score = find_common_and_extra_elements(ground_truth_triples, model_response_triples)\n",
        "\n",
        "        accuracy_results['precision'].append(precision)\n",
        "        accuracy_results['recall'].append(recall)\n",
        "        accuracy_results['f1_score'].append(f1_score)\n",
        "        response_results['results'].append(response_text)\n",
        "\n",
        "\n",
        "        print(\"Common Triples:\", common)\n",
        "        print(\"Extra Triples:\", extra)\n",
        "        print(f\"Precision: {precision:.2f}, Recall: {recall:.2f}, F1-score: {f1_score:.2f}\")\n",
        "        print(f\"Response: {response_text}\\n\")\n",
        "\n",
        "    mean_metrics = {\n",
        "        'precision': np.mean(accuracy_results['precision']) if accuracy_results['precision'] else 0,\n",
        "        'recall': np.mean(accuracy_results['recall']) if accuracy_results['recall'] else 0,\n",
        "        'f1_score': np.mean(accuracy_results['f1_score']) if accuracy_results['f1_score'] else 0,\n",
        "    }\n",
        "\n",
        "    return accuracy_results, mean_metrics, response_results\n"
      ],
      "metadata": {
        "id": "s12mDqADBUTX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2adBXtVCAcXH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CiRGYoooMZTC"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YhyTyHKkGnab"
      },
      "source": [
        "\n",
        "se1 = LocalSearch1(\n",
        "    llm=llm2,\n",
        "    context_builder=context_builder,\n",
        "    token_encoder=token_encoder,\n",
        "    llm_params=llm_params,\n",
        "    context_builder_params=local_context_params,\n",
        "    response_type=\"triples\",\n",
        ")\n",
        "\n",
        "res1= await se1.asearch(query=query, search_prompt=kgg)\n",
        "res1.response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8tctjrHl6pVn"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}